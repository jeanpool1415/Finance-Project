{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPCA EARNINGS SURPRISE PREDICTION\n",
      "==================================================\n",
      "Loaded data: 911537 observations, 165 features\n",
      "Earnings surprise distribution:\n",
      "earnings_surprise\n",
      "1.0    310507\n",
      "NaN    307811\n",
      "0.0    293219\n",
      "Name: count, dtype: int64\n",
      "Using 148 characteristic variables for IPCA\n",
      "\n",
      "==================================================\n",
      "SELECTING OPTIMAL NUMBER OF FACTORS (K)\n",
      "==================================================\n",
      "\n",
      "Testing K = 4...\n",
      "Data after filtering: 403294 observations\n",
      "Using 295 dates with sufficient cross-section\n",
      "Final IPCA dataset: 127199 observations\n",
      "Running out-of-sample IPCA estimation...\n",
      "iters 50: tol = 0.10351827181307294\n",
      "iters 100: tol = 0.01997162832782351\n",
      "iters 150: tol = 0.0004490765502374039\n",
      "iters 50: tol = 0.0009626866873225781\n",
      "iters 50: tol = 0.0002038675146982527\n",
      "iters 50: tol = 0.0002737770873626477\n",
      "iters 50: tol = 0.00016373562731397673\n",
      "iters 50: tol = 0.0008937911715094504\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 367\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Optimal K used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 323\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    320\u001b[0m data, char_vars \u001b[38;5;241m=\u001b[39m prepare_earnings_surprise_data(filepath)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Select optimal K\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m optimal_k, k_results \u001b[38;5;241m=\u001b[39m \u001b[43mselect_optimal_k\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Test K from 4 to 15\u001b[39;49;00m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_obs_per_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moos_start_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2012\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moos_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Run IPCA analysis with optimal K\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning final analysis with K = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 133\u001b[0m, in \u001b[0;36mselect_optimal_k\u001b[0;34m(data, char_vars, k_range, min_obs_per_date, oos_start_year, oos_window)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTesting K = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Run IPCA with current K\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     results, ipca, ipca_input_k \u001b[38;5;241m=\u001b[39m \u001b[43mrun_ipca_earnings_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_obs_per_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_obs_per_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moos_start_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moos_start_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moos_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moos_window\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Quick evaluation\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     prediction_results \u001b[38;5;241m=\u001b[39m evaluate_ipca_factors_for_prediction(\n\u001b[1;32m    140\u001b[0m         results, ipca_input_k, ipca, K\u001b[38;5;241m=\u001b[39mk, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[7], line 102\u001b[0m, in \u001b[0;36mrun_ipca_earnings_prediction\u001b[0;34m(data, char_vars, K, min_obs_per_date, oos_start_year, oos_window)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning out-of-sample IPCA estimation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Out-of-sample estimation\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mipca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOOS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOOS_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecursive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOOS_window_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moos_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mR_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdispIters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdispItersInt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning in-sample IPCA estimation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ipca_classes_update.py:749\u001b[0m, in \u001b[0;36mIPCA_v1.fit\u001b[0;34m(self, K, OOS, gFac, normalization_choice, normalization_choice_specs, OOS_window, OOS_window_specs, factor_mean, R_fit, Beta_fit, dispIters, minTol, maxIters, F_names, G_names, R2_bench, dispItersInt)\u001b[0m\n\u001b[1;32m    746\u001b[0m iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# for first t, Gamma0 will be from _svd_initial outside loop; for subsequent t, will be that last\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m#  Gamma0 obtained in the previous t's iterative \"while\" stmt\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m Gamma1, Factor1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_als_estimation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGamma0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGamma0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgFac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgFac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# make _gFac for ndarray-based\u001b[39;49;00m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_choice_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization_choice_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m tolGam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(Gamma1 \u001b[38;5;241m-\u001b[39m Gamma0))\n\u001b[1;32m    761\u001b[0m tolFac \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(Factor1 \u001b[38;5;241m-\u001b[39m Factor0))\n",
      "File \u001b[0;32m~/ipca_classes_update.py:998\u001b[0m, in \u001b[0;36mIPCA_v1._linear_als_estimation\u001b[0;34m(self, Gamma0, K, M, KM, normalization_choice, normalization_choice_specs, gFac, Dates)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m Dates:\n\u001b[1;32m    996\u001b[0m     numer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mkron(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[t]\u001b[38;5;241m.\u001b[39mvalues, Factor[:, ct]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNts[t]\n\u001b[1;32m    997\u001b[0m     denom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 998\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFactor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFactor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNts[t]\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m     ct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# # ndarray-based\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# ct=0\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# for t in Dates:\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m#     numer += np.kron(self._X[:, t], Factor[:, ct]) * self.Nts[t]\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m#     denom += np.kron(self._W[:, :, t], np.outer(Factor[:, ct], Factor[:, ct])) * self.Nts[t]\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m#     ct+=1\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numpy/lib/shape_base.py:1173\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1171\u001b[0m b_arr \u001b[38;5;241m=\u001b[39m expand_dims(b_arr, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, nd\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;66;03m# In case of `mat`, convert result to `array`\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_any_mat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# Reshape back\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(_nx\u001b[38;5;241m.\u001b[39mmultiply(as_, bs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ipca_classes_update import IPCA_v1\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_earnings_surprise_data(filepath):\n",
    "    \"\"\"\n",
    "    Load and prepare data for earnings surprise prediction using IPCA\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(filepath)\n",
    "    print(f\"Loaded data: {data.shape[0]} observations, {data.shape[1]} features\")\n",
    "    \n",
    "    # Create date column and sort\n",
    "    data['date'] = pd.to_datetime(data[['year', 'month']].assign(day=1))\n",
    "    data = data.sort_values(['date', 'permno']).reset_index(drop=True)\n",
    "    \n",
    "    # Create earnings surprise binary target\n",
    "    # 1 if actual EPS > median estimate, 0 otherwise\n",
    "    data['earnings_surprise'] = np.where(\n",
    "        (data['eps_actual'].notna()) & (data['eps_medest'].notna()),\n",
    "        (data['eps_actual'] > data['eps_medest']).astype(int),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    print(f\"Earnings surprise distribution:\")\n",
    "    print(data['earnings_surprise'].value_counts(dropna=False))\n",
    "    \n",
    "    # Define characteristic variables (excluding forward-looking and identifiers)\n",
    "    exclude_vars = [\n",
    "        # Target/forward-looking variables\n",
    "        'ret_eom', 'stock_exret', 'earnings_surprise',\n",
    "        'eps_medest', 'eps_meanest', 'eps_stdevest', 'eps_actual',\n",
    "        \n",
    "        # Identifiers and date variables\n",
    "        'permno', 'CUSIP', 'stock_ticker', 'comp_name', \n",
    "        'year', 'month', 'date', 'SHRCD', 'EXCHCD',\n",
    "        \n",
    "        # Market-wide variables\n",
    "        'RF', 'size_port'\n",
    "    ]\n",
    "    \n",
    "    # Get characteristic variables\n",
    "    char_vars = [col for col in data.columns if col not in exclude_vars]\n",
    "    print(f\"Using {len(char_vars)} characteristic variables for IPCA\")\n",
    "    \n",
    "    return data, char_vars\n",
    "\n",
    "def run_ipca_earnings_prediction(data, char_vars, K=6, min_obs_per_date=50, \n",
    "                                oos_start_year=2010, oos_window=60):\n",
    "    \"\"\"\n",
    "    Run IPCA analysis for earnings surprise prediction\n",
    "    \"\"\"\n",
    "    # Filter data with valid earnings surprise and sufficient characteristics\n",
    "    valid_data = data.dropna(subset=['earnings_surprise'] + char_vars[:20])  # Require at least 20 non-missing chars\n",
    "    print(f\"Data after filtering: {valid_data.shape[0]} observations\")\n",
    "    \n",
    "    # Filter dates with sufficient cross-section\n",
    "    date_counts = valid_data.groupby('date').size()\n",
    "    valid_dates = date_counts[date_counts >= min_obs_per_date].index\n",
    "    valid_data = valid_data[valid_data['date'].isin(valid_dates)]\n",
    "    print(f\"Using {len(valid_dates)} dates with sufficient cross-section\")\n",
    "    \n",
    "    # Create multi-index dataset for IPCA\n",
    "    ipca_data = valid_data.set_index(['date', 'permno'])[['earnings_surprise'] + char_vars]\n",
    "    \n",
    "    # Handle missing values by forward-filling within each stock\n",
    "    ipca_data = ipca_data.groupby(level=1).fillna(method='ffill')\n",
    "    \n",
    "    # Rank transform characteristics to [-0.5, 0.5] by date\n",
    "    char_data = ipca_data[char_vars].copy()\n",
    "    for date in char_data.index.get_level_values(0).unique():\n",
    "        date_mask = char_data.index.get_level_values(0) == date\n",
    "        for var in char_vars:\n",
    "            if char_data.loc[date_mask, var].notna().sum() > 10:  # Sufficient non-missing\n",
    "                ranks = char_data.loc[date_mask, var].rank(method='dense') - 1\n",
    "                max_rank = ranks.max()\n",
    "                if max_rank > 0:\n",
    "                    char_data.loc[date_mask, var] = (ranks / max_rank) - 0.5\n",
    "                else:\n",
    "                    char_data.loc[date_mask, var] = 0\n",
    "    \n",
    "    # Combine target with transformed characteristics\n",
    "    ipca_input = pd.concat([ipca_data[['earnings_surprise']], char_data], axis=1)\n",
    "    ipca_input = ipca_input.dropna()\n",
    "    \n",
    "    print(f\"Final IPCA dataset: {ipca_input.shape[0]} observations\")\n",
    "    \n",
    "    # Initialize IPCA\n",
    "    ipca = IPCA_v1(ipca_input, return_column='earnings_surprise', add_constant=True)\n",
    "    \n",
    "    # Split data for OOS analysis\n",
    "    oos_dates = ipca_input.index.get_level_values(0) >= pd.to_datetime(f'{oos_start_year}-01-01')\n",
    "    \n",
    "    if oos_dates.sum() > 0:\n",
    "        print(\"Running out-of-sample IPCA estimation...\")\n",
    "        # Out-of-sample estimation\n",
    "        results = ipca.fit(\n",
    "            K=K, \n",
    "            OOS=True, \n",
    "            OOS_window='recursive', \n",
    "            OOS_window_specs=oos_window,\n",
    "            R_fit=True,\n",
    "            dispIters=True,\n",
    "            dispItersInt=50\n",
    "        )\n",
    "    else:\n",
    "        print(\"Running in-sample IPCA estimation...\")\n",
    "        # In-sample estimation\n",
    "        results = ipca.fit(K=K, R_fit=True, dispIters=True)\n",
    "    \n",
    "    return results, ipca, ipca_input\n",
    "\n",
    "def select_optimal_k(data, char_vars, k_range=range(4, 16), \n",
    "                    min_obs_per_date=100, oos_start_year=2010, oos_window=60):\n",
    "    \"\"\"\n",
    "    Select optimal number of factors K using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SELECTING OPTIMAL NUMBER OF FACTORS (K)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    k_results = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"\\nTesting K = {k}...\")\n",
    "        try:\n",
    "            # Run IPCA with current K\n",
    "            results, ipca, ipca_input_k = run_ipca_earnings_prediction(\n",
    "                data, char_vars, K=k, min_obs_per_date=min_obs_per_date,\n",
    "                oos_start_year=oos_start_year, oos_window=oos_window\n",
    "            )\n",
    "            \n",
    "            # Quick evaluation\n",
    "            prediction_results = evaluate_ipca_factors_for_prediction(\n",
    "                results, ipca_input_k, ipca, K=k, verbose=False\n",
    "            )\n",
    "            \n",
    "            if prediction_results and 'metrics' in prediction_results:\n",
    "                auc = prediction_results['metrics']['auc']\n",
    "                r2_managed = results['xfits']['R2_Total']\n",
    "                r2_returns = results['rfits']['R2_Total']\n",
    "                \n",
    "                k_results[k] = {\n",
    "                    'auc': auc,\n",
    "                    'r2_managed': r2_managed,\n",
    "                    'r2_returns': r2_returns,\n",
    "                    'score': auc  # Primary metric for selection\n",
    "                }\n",
    "                \n",
    "                print(f\"  AUC: {auc:.3f}, R²(managed): {r2_managed:.3f}, R²(returns): {r2_returns:.3f}\")\n",
    "            else:\n",
    "                k_results[k] = {'auc': 0, 'r2_managed': 0, 'r2_returns': 0, 'score': 0}\n",
    "                print(f\"  Failed to evaluate K={k}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with K={k}: {str(e)}\")\n",
    "            k_results[k] = {'auc': 0, 'r2_managed': 0, 'r2_returns': 0, 'score': 0}\n",
    "    \n",
    "    # Find optimal K\n",
    "    if k_results:\n",
    "        optimal_k = max(k_results.keys(), key=lambda x: k_results[x]['score'])\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*30)\n",
    "        print(\"K SELECTION RESULTS:\")\n",
    "        print(\"=\"*30)\n",
    "        for k in sorted(k_results.keys()):\n",
    "            metrics = k_results[k]\n",
    "            marker = \" ← OPTIMAL\" if k == optimal_k else \"\"\n",
    "            print(f\"K={k:2d}: AUC={metrics['auc']:.3f}, R²(mgd)={metrics['r2_managed']:.3f}, R²(ret)={metrics['r2_returns']:.3f}{marker}\")\n",
    "        \n",
    "        print(f\"\\nSelected optimal K = {optimal_k}\")\n",
    "        return optimal_k, k_results\n",
    "    else:\n",
    "        print(\"No valid results found, defaulting to K=6\")\n",
    "        return 6, {}\n",
    "\n",
    "def evaluate_ipca_factors_for_prediction(results, ipca_input, ipca, K=6, verbose=True):\n",
    "    \"\"\"\n",
    "    Use IPCA factors to predict earnings surprise\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATING IPCA FACTORS FOR EARNINGS PREDICTION\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # Extract factor loadings and create factor scores\n",
    "    gamma = results['Gamma']\n",
    "    if isinstance(gamma.index, pd.MultiIndex):\n",
    "        # OOS case - use latest Gamma\n",
    "        latest_date = gamma.index.get_level_values(0).max()\n",
    "        gamma_final = gamma.loc[latest_date]\n",
    "    else:\n",
    "        # In-sample case\n",
    "        gamma_final = gamma\n",
    "    \n",
    "    print(f\"Factor loadings shape: {gamma_final.shape}\")\n",
    "    if verbose:\n",
    "        print(f\"Top characteristics for each factor:\")\n",
    "        for i in range(min(K, gamma_final.shape[1])):\n",
    "            factor_name = gamma_final.columns[i]\n",
    "            top_chars = gamma_final.iloc[:, i].abs().nlargest(5)\n",
    "            print(f\"\\nFactor {factor_name}:\")\n",
    "            for char, loading in top_chars.items():\n",
    "                print(f\"  {char}: {loading:.3f}\")\n",
    "    \n",
    "    # Create factor scores for prediction\n",
    "    char_vars = gamma_final.index.tolist()\n",
    "    if 'Constant' in char_vars:\n",
    "        char_vars.remove('Constant')\n",
    "    \n",
    "    # Calculate factor scores\n",
    "    factor_scores_list = []\n",
    "    \n",
    "    for date in ipca_input.index.get_level_values(0).unique():\n",
    "        date_data = ipca_input.loc[date]\n",
    "        if date_data.shape[0] > 10:  # Sufficient observations\n",
    "            # Get characteristics matrix\n",
    "            X = date_data[char_vars].values\n",
    "            # Add constant\n",
    "            X_with_const = np.column_stack([X, np.ones(X.shape[0])])\n",
    "            \n",
    "            # Calculate factor scores\n",
    "            factor_scores = X_with_const @ gamma_final.values\n",
    "            \n",
    "            # Create DataFrame\n",
    "            factor_df = pd.DataFrame(\n",
    "                factor_scores, \n",
    "                index=date_data.index,\n",
    "                columns=gamma_final.columns\n",
    "            )\n",
    "            factor_df['date'] = date\n",
    "            factor_df['earnings_surprise'] = date_data['earnings_surprise'].values\n",
    "            factor_scores_list.append(factor_df)\n",
    "    \n",
    "    if len(factor_scores_list) == 0:\n",
    "        if verbose:\n",
    "            print(\"No valid data for factor score calculation\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all factor scores\n",
    "    all_factor_scores = pd.concat(factor_scores_list, ignore_index=True)\n",
    "    all_factor_scores = all_factor_scores.dropna()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFactor scores dataset: {all_factor_scores.shape[0]} observations\")\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_data = all_factor_scores[all_factor_scores['date'] < pd.to_datetime('2015-01-01')]\n",
    "    test_data = all_factor_scores[all_factor_scores['date'] >= pd.to_datetime('2015-01-01')]\n",
    "    \n",
    "    if len(train_data) == 0 or len(test_data) == 0:\n",
    "        if verbose:\n",
    "            print(\"Insufficient data for train/test split\")\n",
    "        return all_factor_scores\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Train set: {len(train_data)} observations\")\n",
    "        print(f\"Test set: {len(test_data)} observations\")\n",
    "    \n",
    "    # Prepare features (exclude date and target)\n",
    "    factor_cols = gamma_final.columns.tolist()\n",
    "    X_train = train_data[factor_cols]\n",
    "    y_train = train_data['earnings_surprise']\n",
    "    X_test = test_data[factor_cols]\n",
    "    y_test = test_data['earnings_surprise']\n",
    "    \n",
    "    # Train logistic regression\n",
    "# With this:\n",
    "    xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPREDICTION RESULTS:\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall: {recall:.3f}\")\n",
    "        print(f\"F1-Score: {f1:.3f}\")\n",
    "        print(f\"AUC-ROC: {auc:.3f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        print(f\"\\nFACTOR IMPORTANCE (Logistic Regression Coefficients):\")\n",
    "        for factor, coef in zip(factor_cols, lr.coef_[0]):\n",
    "            print(f\"{factor}: {coef:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'factor_scores': all_factor_scores,\n",
    "        'gamma': gamma_final,\n",
    "        'model': lr,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"IPCA EARNINGS SURPRISE PREDICTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    filepath = '/teamspace/studios/this_studio/goup_project_sample_v3.csv'  # Replace with your actual file path\n",
    "    data, char_vars = prepare_earnings_surprise_data(filepath)\n",
    "    \n",
    "    # Select optimal K\n",
    "    optimal_k, k_results = select_optimal_k(\n",
    "        data, char_vars,\n",
    "        k_range=range(4, 16),  # Test K from 4 to 15\n",
    "        min_obs_per_date=100,\n",
    "        oos_start_year=2012,\n",
    "        oos_window=60\n",
    "    )\n",
    "    \n",
    "    # Run IPCA analysis with optimal K\n",
    "    print(f\"\\nRunning final analysis with K = {optimal_k}...\")\n",
    "    results, ipca, ipca_input = run_ipca_earnings_prediction(\n",
    "        data, char_vars, \n",
    "        K=optimal_k,\n",
    "        min_obs_per_date=100,\n",
    "        oos_start_year=2012,\n",
    "        oos_window=60\n",
    "    )\n",
    "    \n",
    "    # Print IPCA results\n",
    "    print(f\"\\nFINAL IPCA ESTIMATION RESULTS (K={optimal_k}):\")\n",
    "    print(f\"Managed Portfolio R²: {results['xfits']['R2_Total']:.3f}\")\n",
    "    print(f\"Returns R²: {results['rfits']['R2_Total']:.3f}\")\n",
    "    \n",
    "    # Detailed evaluation with optimal K\n",
    "    prediction_results = evaluate_ipca_factors_for_prediction(\n",
    "        results, ipca_input, ipca, K=optimal_k, verbose=True\n",
    "    )\n",
    "    \n",
    "    if prediction_results:\n",
    "        # Save results\n",
    "        prediction_results['factor_scores'].to_csv('ipca_factor_scores.csv', index=False)\n",
    "        results['Gamma'].to_csv('ipca_factor_loadings.csv')\n",
    "        \n",
    "        # Save K selection results\n",
    "        k_results_df = pd.DataFrame(k_results).T\n",
    "        k_results_df.to_csv('k_selection_results.csv')\n",
    "        \n",
    "        print(f\"\\nResults saved:\")\n",
    "        print(f\"- Factor scores: ipca_factor_scores.csv\")\n",
    "        print(f\"- Factor loadings: ipca_factor_loadings.csv\") \n",
    "        print(f\"- K selection results: k_selection_results.csv\")\n",
    "        print(f\"- Optimal K used: {optimal_k}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from ipca_classes_update import IPCA_v1\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_earnings_surprise_data(filepath):\n",
    "    \"\"\"\n",
    "    Load and prepare data for earnings surprise prediction using IPCA\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(filepath)\n",
    "    print(f\"Loaded data: {data.shape[0]} observations, {data.shape[1]} features\")\n",
    "    \n",
    "    # Create date column and sort\n",
    "    data['date'] = pd.to_datetime(data[['year', 'month']].assign(day=1))\n",
    "    data = data.sort_values(['date', 'permno']).reset_index(drop=True)\n",
    "    \n",
    "    # Create earnings surprise binary target\n",
    "    # 1 if actual EPS > median estimate, 0 otherwise\n",
    "    data['earnings_surprise'] = np.where(\n",
    "        (data['eps_actual'].notna()) & (data['eps_medest'].notna()),\n",
    "        (data['eps_actual'] > data['eps_medest']).astype(int),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    print(f\"Earnings surprise distribution:\")\n",
    "    print(data['earnings_surprise'].value_counts(dropna=False))\n",
    "    \n",
    "    # Define characteristic variables (excluding forward-looking and identifiers)\n",
    "    exclude_vars = [\n",
    "        # Target/forward-looking variables\n",
    "        'ret_eom', 'stock_exret', 'earnings_surprise',\n",
    "        'eps_medest', 'eps_meanest', 'eps_stdevest', 'eps_actual',\n",
    "        \n",
    "        # Identifiers and date variables\n",
    "        'permno', 'CUSIP', 'stock_ticker', 'comp_name', \n",
    "        'year', 'month', 'date', 'SHRCD', 'EXCHCD',\n",
    "        \n",
    "        # Market-wide variables\n",
    "        'RF', 'size_port'\n",
    "    ]\n",
    "    \n",
    "    # Get characteristic variables\n",
    "    char_vars = [col for col in data.columns if col not in exclude_vars]\n",
    "    print(f\"Using {len(char_vars)} characteristic variables for IPCA\")\n",
    "    \n",
    "    return data, char_vars\n",
    "\n",
    "def run_ipca_earnings_prediction(data, char_vars, K=6, min_obs_per_date=50, \n",
    "                                oos_start_year=2010, oos_window=60):\n",
    "    \"\"\"\n",
    "    Run IPCA analysis for earnings surprise prediction\n",
    "    \"\"\"\n",
    "    # Filter data with valid earnings surprise and sufficient characteristics\n",
    "    valid_data = data.dropna(subset=['earnings_surprise'] + char_vars[:20])  # Require at least 20 non-missing chars\n",
    "    print(f\"Data after filtering: {valid_data.shape[0]} observations\")\n",
    "    \n",
    "    # Filter dates with sufficient cross-section\n",
    "    date_counts = valid_data.groupby('date').size()\n",
    "    valid_dates = date_counts[date_counts >= min_obs_per_date].index\n",
    "    valid_data = valid_data[valid_data['date'].isin(valid_dates)]\n",
    "    print(f\"Using {len(valid_dates)} dates with sufficient cross-section\")\n",
    "    \n",
    "    # Create multi-index dataset for IPCA\n",
    "    ipca_data = valid_data.set_index(['date', 'permno'])[['earnings_surprise'] + char_vars]\n",
    "    \n",
    "    # Handle missing values by forward-filling within each stock\n",
    "    ipca_data = ipca_data.groupby(level=1).fillna(method='ffill')\n",
    "    \n",
    "    # Rank transform characteristics to [-0.5, 0.5] by date\n",
    "    char_data = ipca_data[char_vars].copy()\n",
    "    for date in char_data.index.get_level_values(0).unique():\n",
    "        date_mask = char_data.index.get_level_values(0) == date\n",
    "        for var in char_vars:\n",
    "            if char_data.loc[date_mask, var].notna().sum() > 10:  # Sufficient non-missing\n",
    "                ranks = char_data.loc[date_mask, var].rank(method='dense') - 1\n",
    "                max_rank = ranks.max()\n",
    "                if max_rank > 0:\n",
    "                    char_data.loc[date_mask, var] = (ranks / max_rank) - 0.5\n",
    "                else:\n",
    "                    char_data.loc[date_mask, var] = 0\n",
    "    \n",
    "    # Combine target with transformed characteristics\n",
    "    ipca_input = pd.concat([ipca_data[['earnings_surprise']], char_data], axis=1)\n",
    "    ipca_input = ipca_input.dropna()\n",
    "    \n",
    "    print(f\"Final IPCA dataset: {ipca_input.shape[0]} observations\")\n",
    "    \n",
    "    # Initialize IPCA\n",
    "    ipca = IPCA_v1(ipca_input, return_column='earnings_surprise', add_constant=True)\n",
    "    \n",
    "    # Split data for OOS analysis\n",
    "    oos_dates = ipca_input.index.get_level_values(0) >= pd.to_datetime(f'{oos_start_year}-01-01')\n",
    "    \n",
    "    if oos_dates.sum() > 0:\n",
    "        print(\"Running out-of-sample IPCA estimation...\")\n",
    "        # Out-of-sample estimation\n",
    "        results = ipca.fit(\n",
    "            K=K, \n",
    "            OOS=True, \n",
    "            OOS_window='recursive', \n",
    "            OOS_window_specs=oos_window,\n",
    "            R_fit=True,\n",
    "            dispIters=True,\n",
    "            dispItersInt=50\n",
    "        )\n",
    "    else:\n",
    "        print(\"Running in-sample IPCA estimation...\")\n",
    "        # In-sample estimation\n",
    "        results = ipca.fit(K=K, R_fit=True, dispIters=True)\n",
    "    \n",
    "    return results, ipca, ipca_input\n",
    "\n",
    "def select_optimal_k(data, char_vars, k_range=range(4, 16), \n",
    "                    min_obs_per_date=100, oos_start_year=2010, oos_window=60):\n",
    "    \"\"\"\n",
    "    Select optimal number of factors K using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SELECTING OPTIMAL NUMBER OF FACTORS (K)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    k_results = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"\\nTesting K = {k}...\")\n",
    "        try:\n",
    "            # Run IPCA with current K\n",
    "            results, ipca, ipca_input_k = run_ipca_earnings_prediction(\n",
    "                data, char_vars, K=k, min_obs_per_date=min_obs_per_date,\n",
    "                oos_start_year=oos_start_year, oos_window=oos_window\n",
    "            )\n",
    "            \n",
    "            # Quick evaluation\n",
    "            prediction_results = evaluate_ipca_factors_for_prediction(\n",
    "                results, ipca_input_k, ipca, K=k, verbose=False\n",
    "            )\n",
    "            \n",
    "            if prediction_results and 'metrics' in prediction_results:\n",
    "                auc = prediction_results['metrics']['auc']\n",
    "                r2_managed = results['xfits']['R2_Total']\n",
    "                r2_returns = results['rfits']['R2_Total']\n",
    "                \n",
    "                k_results[k] = {\n",
    "                    'auc': auc,\n",
    "                    'r2_managed': r2_managed,\n",
    "                    'r2_returns': r2_returns,\n",
    "                    'score': auc  # Primary metric for selection\n",
    "                }\n",
    "                \n",
    "                print(f\"  AUC: {auc:.3f}, R²(managed): {r2_managed:.3f}, R²(returns): {r2_returns:.3f}\")\n",
    "            else:\n",
    "                k_results[k] = {'auc': 0, 'r2_managed': 0, 'r2_returns': 0, 'score': 0}\n",
    "                print(f\"  Failed to evaluate K={k}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with K={k}: {str(e)}\")\n",
    "            k_results[k] = {'auc': 0, 'r2_managed': 0, 'r2_returns': 0, 'score': 0}\n",
    "    \n",
    "    # Find optimal K\n",
    "    if k_results:\n",
    "        optimal_k = max(k_results.keys(), key=lambda x: k_results[x]['score'])\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*30)\n",
    "        print(\"K SELECTION RESULTS:\")\n",
    "        print(\"=\"*30)\n",
    "        for k in sorted(k_results.keys()):\n",
    "            metrics = k_results[k]\n",
    "            marker = \" ← OPTIMAL\" if k == optimal_k else \"\"\n",
    "            print(f\"K={k:2d}: AUC={metrics['auc']:.3f}, R²(mgd)={metrics['r2_managed']:.3f}, R²(ret)={metrics['r2_returns']:.3f}{marker}\")\n",
    "        \n",
    "        print(f\"\\nSelected optimal K = {optimal_k}\")\n",
    "        return optimal_k, k_results\n",
    "    else:\n",
    "        print(\"No valid results found, defaulting to K=6\")\n",
    "        return 6, {}\n",
    "\n",
    "def evaluate_ipca_factors_for_prediction(results, ipca_input, ipca, K=6, verbose=True):\n",
    "    \"\"\"\n",
    "    Use IPCA factors to predict earnings surprise\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATING IPCA FACTORS FOR EARNINGS PREDICTION\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # Extract factor loadings and create factor scores\n",
    "    gamma = results['Gamma']\n",
    "    if isinstance(gamma.index, pd.MultiIndex):\n",
    "        # OOS case - use latest Gamma\n",
    "        latest_date = gamma.index.get_level_values(0).max()\n",
    "        gamma_final = gamma.loc[latest_date]\n",
    "    else:\n",
    "        # In-sample case\n",
    "        gamma_final = gamma\n",
    "    \n",
    "    print(f\"Factor loadings shape: {gamma_final.shape}\")\n",
    "    if verbose:\n",
    "        print(f\"Top characteristics for each factor:\")\n",
    "        for i in range(min(K, gamma_final.shape[1])):\n",
    "            factor_name = gamma_final.columns[i]\n",
    "            top_chars = gamma_final.iloc[:, i].abs().nlargest(5)\n",
    "            print(f\"\\nFactor {factor_name}:\")\n",
    "            for char, loading in top_chars.items():\n",
    "                print(f\"  {char}: {loading:.3f}\")\n",
    "    \n",
    "    # Create factor scores for prediction\n",
    "    char_vars = gamma_final.index.tolist()\n",
    "    if 'Constant' in char_vars:\n",
    "        char_vars.remove('Constant')\n",
    "    \n",
    "    # Calculate factor scores\n",
    "    factor_scores_list = []\n",
    "    \n",
    "    for date in ipca_input.index.get_level_values(0).unique():\n",
    "        date_data = ipca_input.loc[date]\n",
    "        if date_data.shape[0] > 10:  # Sufficient observations\n",
    "            # Get characteristics matrix\n",
    "            X = date_data[char_vars].values\n",
    "            # Add constant\n",
    "            X_with_const = np.column_stack([X, np.ones(X.shape[0])])\n",
    "            \n",
    "            # Calculate factor scores\n",
    "            factor_scores = X_with_const @ gamma_final.values\n",
    "            \n",
    "            # Create DataFrame\n",
    "            factor_df = pd.DataFrame(\n",
    "                factor_scores, \n",
    "                index=date_data.index,\n",
    "                columns=gamma_final.columns\n",
    "            )\n",
    "            factor_df['date'] = date\n",
    "            factor_df['year'] = date.year\n",
    "            factor_df['month'] = date.month\n",
    "            factor_df['permno'] = date_data.index  # Add permno\n",
    "            factor_df['earnings_surprise'] = date_data['earnings_surprise'].values\n",
    "            factor_scores_list.append(factor_df)\n",
    "    \n",
    "    if len(factor_scores_list) == 0:\n",
    "        if verbose:\n",
    "            print(\"No valid data for factor score calculation\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all factor scores\n",
    "    all_factor_scores = pd.concat(factor_scores_list, ignore_index=True)\n",
    "    all_factor_scores = all_factor_scores.dropna()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFactor scores dataset: {all_factor_scores.shape[0]} observations\")\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_data = all_factor_scores[all_factor_scores['date'] < pd.to_datetime('2015-01-01')]\n",
    "    test_data = all_factor_scores[all_factor_scores['date'] >= pd.to_datetime('2015-01-01')]\n",
    "    \n",
    "    if len(train_data) == 0 or len(test_data) == 0:\n",
    "        if verbose:\n",
    "            print(\"Insufficient data for train/test split\")\n",
    "        return all_factor_scores\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Train set: {len(train_data)} observations\")\n",
    "        print(f\"Test set: {len(test_data)} observations\")\n",
    "    \n",
    "    # Prepare features (exclude date and target)\n",
    "    factor_cols = gamma_final.columns.tolist()\n",
    "    X_train = train_data[factor_cols]\n",
    "    y_train = train_data['earnings_surprise']\n",
    "    X_test = test_data[factor_cols]\n",
    "    y_test = test_data['earnings_surprise']\n",
    "    \n",
    "    # Train XGBoost classifier\n",
    "    xgb = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPREDICTION RESULTS:\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall: {recall:.3f}\")\n",
    "        print(f\"F1-Score: {f1:.3f}\")\n",
    "        print(f\"AUC-ROC: {auc:.3f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        print(f\"\\nFACTOR IMPORTANCE (XGBoost):\")\n",
    "        for factor, importance in zip(factor_cols, xgb.feature_importances_):\n",
    "            print(f\"{factor}: {importance:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'factor_scores': all_factor_scores,\n",
    "        'gamma': gamma_final,\n",
    "        'model': xgb,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc\n",
    "        }\n",
    "    }\n",
    "\n",
    "def backtest_ipca_strategy(data, prediction_results, start_year=2015, end_year=2023, \n",
    "                          top_n_stocks=50, hold_period=1):\n",
    "    \"\"\"\n",
    "    Backtest IPCA-based earnings surprise strategy\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BACKTESTING IPCA EARNINGS SURPRISE STRATEGY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get factor scores with predictions\n",
    "    factor_scores = prediction_results['factor_scores'].copy()\n",
    "    model = prediction_results['model']\n",
    "    gamma = prediction_results['gamma']\n",
    "    \n",
    "    # Merge with original data to get returns\n",
    "    factor_scores['date'] = pd.to_datetime(factor_scores[['year', 'month']].assign(day=1))\n",
    "    data_with_dates = data.copy()\n",
    "    data_with_dates['date'] = pd.to_datetime(data_with_dates[['year', 'month']].assign(day=1))\n",
    "    \n",
    "    # Add stock returns to factor scores\n",
    "    backtest_data = factor_scores.merge(\n",
    "        data_with_dates[['permno', 'date', 'stock_exret', 'RF']], \n",
    "        on=['permno', 'date'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Generate predictions for all data\n",
    "    factor_cols = gamma.columns.tolist()\n",
    "    X_all = backtest_data[factor_cols].fillna(0)\n",
    "    backtest_data['pred_proba'] = model.predict_proba(X_all)[:, 1]\n",
    "    backtest_data['pred_binary'] = model.predict(X_all)\n",
    "    \n",
    "    # Create trading periods\n",
    "    trade_periods = []\n",
    "    current_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        trade_periods.append((current_date.year, current_date.month))\n",
    "        current_date += relativedelta(months=1)\n",
    "    \n",
    "    # Portfolio construction and backtesting\n",
    "    portfolio_returns = []\n",
    "    portfolio_compositions = []\n",
    "    benchmark_returns = []\n",
    "    \n",
    "    for i, (year, month) in enumerate(trade_periods[:-hold_period]):\n",
    "        # Selection data (current month)\n",
    "        selection_data = backtest_data[\n",
    "            (backtest_data['year'] == year) & \n",
    "            (backtest_data['month'] == month)\n",
    "        ].copy()\n",
    "        \n",
    "        if selection_data.empty:\n",
    "            portfolio_returns.append(0.0)\n",
    "            benchmark_returns.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Select top stocks based on earnings surprise probability\n",
    "        top_stocks = selection_data.nlargest(top_n_stocks, 'pred_proba')\n",
    "        \n",
    "        if len(top_stocks) == 0:\n",
    "            portfolio_returns.append(0.0)\n",
    "            benchmark_returns.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Equal weight portfolio\n",
    "        weights = np.ones(len(top_stocks)) / len(top_stocks)\n",
    "        \n",
    "        # Get returns for holding period (next month)\n",
    "        hold_year, hold_month = trade_periods[i + hold_period]\n",
    "        return_data = backtest_data[\n",
    "            (backtest_data['year'] == hold_year) & \n",
    "            (backtest_data['month'] == hold_month) &\n",
    "            (backtest_data['permno'].isin(top_stocks['permno']))\n",
    "        ]\n",
    "        \n",
    "        # Calculate portfolio return\n",
    "        if len(return_data) > 0:\n",
    "            # Match weights to available returns\n",
    "            portfolio_return = 0.0\n",
    "            total_weight = 0.0\n",
    "            \n",
    "            for j, (idx, stock) in enumerate(top_stocks.iterrows()):\n",
    "                stock_return_data = return_data[return_data['permno'] == stock['permno']]\n",
    "                if len(stock_return_data) > 0:\n",
    "                    stock_return = stock_return_data['stock_exret'].iloc[0]\n",
    "                    if pd.notna(stock_return):\n",
    "                        portfolio_return += weights[j] * stock_return\n",
    "                        total_weight += weights[j]\n",
    "            \n",
    "            # Normalize if some stocks missing\n",
    "            if total_weight > 0:\n",
    "                portfolio_return = portfolio_return / total_weight\n",
    "            \n",
    "            # Add risk-free rate\n",
    "            rf_rate = return_data['RF'].iloc[0] if len(return_data) > 0 and pd.notna(return_data['RF'].iloc[0]) else 0.0\n",
    "            portfolio_returns.append(portfolio_return + rf_rate)\n",
    "            \n",
    "        else:\n",
    "            portfolio_returns.append(0.0)\n",
    "        \n",
    "        # Benchmark return (risk-free rate for now - can be replaced with market return)\n",
    "        benchmark_returns.append(rf_rate if 'rf_rate' in locals() else 0.0)\n",
    "        \n",
    "        # Store composition\n",
    "        portfolio_compositions.append({\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'num_stocks': len(top_stocks),\n",
    "            'avg_pred_proba': top_stocks['pred_proba'].mean(),\n",
    "            'portfolio_return': portfolio_returns[-1]\n",
    "        })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(portfolio_compositions)\n",
    "    results_df['date'] = pd.to_datetime(results_df[['year', 'month']].assign(day=1))\n",
    "    \n",
    "    return {\n",
    "        'portfolio_returns': portfolio_returns,\n",
    "        'benchmark_returns': benchmark_returns,\n",
    "        'trade_periods': trade_periods[:-hold_period],\n",
    "        'portfolio_compositions': results_df\n",
    "    }\n",
    "\n",
    "def calculate_performance_metrics(portfolio_returns, benchmark_returns, risk_free_rates=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics\n",
    "    \"\"\"\n",
    "    portfolio_returns = pd.Series(portfolio_returns)\n",
    "    benchmark_returns = pd.Series(benchmark_returns)\n",
    "    \n",
    "    if risk_free_rates is None:\n",
    "        risk_free_rates = pd.Series([0.0] * len(portfolio_returns))\n",
    "    else:\n",
    "        risk_free_rates = pd.Series(risk_free_rates)\n",
    "    \n",
    "    # Create DataFrame and remove NaN values\n",
    "    df = pd.DataFrame({\n",
    "        'portfolio': portfolio_returns,\n",
    "        'benchmark': benchmark_returns,\n",
    "        'rf': risk_free_rates\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(df) < 2:\n",
    "        return {k: np.nan for k in [\"Annual Return (%)\", \"Annualized Volatility (%)\", \n",
    "                                   \"Annualized Sharpe Ratio\", \"Max Drawdown (%)\", \n",
    "                                   \"Max Monthly Loss (%)\", \"Beta vs Benchmark\"]}\n",
    "    \n",
    "    # Annualized return\n",
    "    ann_ret = (1 + df['portfolio']).prod()**(12 / len(df)) - 1\n",
    "    \n",
    "    # Annualized volatility\n",
    "    ann_vol = df['portfolio'].std() * np.sqrt(12)\n",
    "    \n",
    "    # Risk-free rate\n",
    "    ann_rf = (1 + df['rf'].mean())**12 - 1\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    sharpe = (ann_ret - ann_rf) / ann_vol if ann_vol > 1e-9 else np.nan\n",
    "    \n",
    "    # Alpha and Beta\n",
    "    excess_portfolio = df['portfolio'] - df['rf']\n",
    "    excess_benchmark = df['benchmark'] - df['rf']\n",
    "    \n",
    "    alpha_monthly, beta = np.nan, np.nan\n",
    "    if len(excess_portfolio) >= 2 and excess_benchmark.std() > 1e-9:\n",
    "        try:\n",
    "            model = sm.OLS(excess_portfolio, sm.add_constant(excess_benchmark)).fit()\n",
    "            alpha_monthly = model.params[0]\n",
    "            beta = model.params[1]\n",
    "        except:\n",
    "            alpha_monthly = excess_portfolio.mean() - excess_benchmark.mean()\n",
    "            beta = np.nan\n",
    "    \n",
    "    ann_alpha = alpha_monthly * 12 if pd.notna(alpha_monthly) else np.nan\n",
    "    \n",
    "    # Drawdown\n",
    "    cumulative_returns = (1 + df['portfolio']).cumprod()\n",
    "    peak = cumulative_returns.expanding().max()\n",
    "    drawdown = (cumulative_returns - peak) / peak\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Max monthly loss\n",
    "    max_loss = df['portfolio'].min()\n",
    "    \n",
    "    # Information Ratio\n",
    "    active_return = df['portfolio'] - df['benchmark']\n",
    "    tracking_error = active_return.std() * np.sqrt(12)\n",
    "    mean_active_return = active_return.mean() * 12\n",
    "    info_ratio = mean_active_return / tracking_error if tracking_error > 1e-9 else np.nan\n",
    "    \n",
    "    return {\n",
    "        \"Annual Return (%)\": ann_ret * 100,\n",
    "        \"Annualized Volatility (%)\": ann_vol * 100,\n",
    "        \"Annualized Sharpe Ratio\": sharpe,\n",
    "        \"Annualized Alpha vs Benchmark (%)\": ann_alpha * 100,\n",
    "        \"Beta vs Benchmark\": beta,\n",
    "        \"Max Drawdown (%)\": max_drawdown * 100,\n",
    "        \"Max Monthly Loss (%)\": max_loss * 100,\n",
    "        \"Annualized Information Ratio vs Benchmark\": info_ratio,\n",
    "        \"Annualized Tracking Error vs Benchmark (%)\": tracking_error * 100\n",
    "    }\n",
    "\n",
    "def create_performance_summary(backtest_results, strategy_name=\"IPCA_Earnings_Strategy\"):\n",
    "    \"\"\"\n",
    "    Create performance summary table\n",
    "    \"\"\"\n",
    "    portfolio_returns = backtest_results['portfolio_returns']\n",
    "    benchmark_returns = backtest_results['benchmark_returns']\n",
    "    \n",
    "    # Calculate metrics for both strategy and benchmark\n",
    "    strategy_metrics = calculate_performance_metrics(portfolio_returns, benchmark_returns)\n",
    "    benchmark_metrics = calculate_performance_metrics(benchmark_returns, benchmark_returns)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Benchmark': benchmark_metrics,\n",
    "        strategy_name: strategy_metrics\n",
    "    })\n",
    "    \n",
    "    # Format numbers\n",
    "    for col in metrics_df.columns:\n",
    "        for idx in metrics_df.index:\n",
    "            val = metrics_df.loc[idx, col]\n",
    "            if pd.isna(val):\n",
    "                metrics_df.loc[idx, col] = 'nan'\n",
    "            elif 'Ratio' in idx:\n",
    "                metrics_df.loc[idx, col] = f\"{val:.3f}\"\n",
    "            elif 'Beta' in idx:\n",
    "                metrics_df.loc[idx, col] = f\"{val:.3f}\"\n",
    "            else:\n",
    "                metrics_df.loc[idx, col] = f\"{val:.2f}\"\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def plot_cumulative_returns(backtest_results, strategy_name=\"IPCA_Earnings_Strategy\"):\n",
    "    \"\"\"\n",
    "    Plot cumulative returns\n",
    "    \"\"\"\n",
    "    portfolio_returns = pd.Series(backtest_results['portfolio_returns'])\n",
    "    benchmark_returns = pd.Series(backtest_results['benchmark_returns'])\n",
    "    trade_periods = backtest_results['trade_periods']\n",
    "    \n",
    "    # Create date index\n",
    "    dates = [datetime(year, month, 1) for year, month in trade_periods]\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cum_portfolio = (1 + portfolio_returns).cumprod() * 100\n",
    "    cum_benchmark = (1 + benchmark_returns).cumprod() * 100\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    cum_returns_df = pd.DataFrame({\n",
    "        strategy_name: cum_portfolio.values,\n",
    "        'Benchmark': cum_benchmark.values\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    cum_returns_df.plot(kind='line')\n",
    "    plt.title('Cumulative Returns Comparison')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return Index (Base = 100)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cum_returns_df\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"IPCA EARNINGS SURPRISE PREDICTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    filepath = 'your_dataset.csv'  # Replace with your actual file path\n",
    "    data, char_vars = prepare_earnings_surprise_data(filepath)\n",
    "    \n",
    "    # Select optimal K\n",
    "    optimal_k, k_results = select_optimal_k(\n",
    "        data, char_vars,\n",
    "        k_range=range(4, 16),  # Test K from 4 to 15\n",
    "        min_obs_per_date=100,\n",
    "        oos_start_year=2012,\n",
    "        oos_window=60\n",
    "    )\n",
    "    \n",
    "    # Run IPCA analysis with optimal K\n",
    "    print(f\"\\nRunning final analysis with K = {optimal_k}...\")\n",
    "    results, ipca, ipca_input = run_ipca_earnings_prediction(\n",
    "        data, char_vars, \n",
    "        K=optimal_k,\n",
    "        min_obs_per_date=100,\n",
    "        oos_start_year=2012,\n",
    "        oos_window=60\n",
    "    )\n",
    "    \n",
    "    # Print IPCA results\n",
    "    print(f\"\\nFINAL IPCA ESTIMATION RESULTS (K={optimal_k}):\")\n",
    "    print(f\"Managed Portfolio R²: {results['xfits']['R2_Total']:.3f}\")\n",
    "    print(f\"Returns R²: {results['rfits']['R2_Total']:.3f}\")\n",
    "    \n",
    "    # Detailed evaluation with optimal K\n",
    "    prediction_results = evaluate_ipca_factors_for_prediction(\n",
    "        results, ipca_input, ipca, K=optimal_k, verbose=True\n",
    "    )\n",
    "    \n",
    "    if prediction_results:\n",
    "        # Save results\n",
    "        prediction_results['factor_scores'].to_csv('ipca_factor_scores.csv', index=False)\n",
    "        results['Gamma'].to_csv('ipca_factor_loadings.csv')\n",
    "        \n",
    "        # Save K selection results\n",
    "        k_results_df = pd.DataFrame(k_results).T\n",
    "        k_results_df.to_csv('k_selection_results.csv')\n",
    "        \n",
    "        print(f\"\\nResults saved:\")\n",
    "        print(f\"- Factor scores: ipca_factor_scores.csv\")\n",
    "        print(f\"- Factor loadings: ipca_factor_loadings.csv\") \n",
    "        print(f\"- K selection results: k_selection_results.csv\")\n",
    "        print(f\"- Optimal K used: {optimal_k}\")\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 911537 observations, 165 features\n",
      "Earnings surprise distribution:\n",
      "earnings_surprise\n",
      "1.0    310507\n",
      "NaN    307811\n",
      "0.0    293219\n",
      "Name: count, dtype: int64\n",
      "Using 148 characteristic variables for IPCA\n",
      "Data after filtering: 403294 observations\n",
      "Using 295 dates with sufficient cross-section\n",
      "Final IPCA dataset: 127199 observations\n",
      "Running out-of-sample IPCA estimation...\n",
      "iters 50: tol = 0.08065946189690842\n",
      "iters 100: tol = 0.06626441854350565\n",
      "iters 150: tol = 0.004592182742092721\n",
      "iters 200: tol = 0.0005191526454617623\n",
      "iters 50: tol = 0.037003190546523346\n",
      "iters 100: tol = 0.030077222830379502\n",
      "iters 150: tol = 0.015600795422230584\n",
      "iters 200: tol = 0.016244122876550054\n",
      "iters 250: tol = 0.016137740145162005\n",
      "iters 300: tol = 0.011160074237905215\n",
      "iters 350: tol = 0.007115623367877633\n",
      "iters 400: tol = 0.004611988097601952\n",
      "iters 450: tol = 0.003059289988542324\n",
      "iters 500: tol = 0.0020819830235951353\n",
      "iters 550: tol = 0.0014440186226952756\n",
      "iters 600: tol = 0.001029008829104533\n",
      "iters 650: tol = 0.0007670173692280319\n",
      "iters 700: tol = 0.0005663844727101519\n",
      "iters 750: tol = 0.0004151763691390975\n",
      "iters 800: tol = 0.0003026370348466756\n",
      "iters 850: tol = 0.00021967690743196489\n",
      "iters 900: tol = 0.00015896089452877016\n",
      "iters 950: tol = 0.00011476199270471499\n",
      "iters 50: tol = 0.010788499320705247\n",
      "iters 100: tol = 0.002053338757406986\n",
      "iters 150: tol = 0.0004946754598947889\n",
      "iters 200: tol = 0.00013229472261788722\n",
      "iters 50: tol = 0.008821327712032367\n",
      "iters 100: tol = 0.0005646622437873727\n",
      "iters 50: tol = 0.007681430903717978\n",
      "iters 100: tol = 0.0015209131295217393\n",
      "iters 150: tol = 0.00046493879707365515\n",
      "iters 200: tol = 0.00015777971737046537\n",
      "iters 50: tol = 0.02465396186111357\n",
      "iters 100: tol = 0.012981116662177161\n",
      "iters 150: tol = 0.019292334273246503\n",
      "iters 200: tol = 0.024572449164001475\n",
      "iters 250: tol = 0.045244216014279415\n",
      "iters 300: tol = 0.009483665201616809\n",
      "iters 350: tol = 0.007888457070984511\n",
      "iters 400: tol = 0.014157602532393398\n",
      "iters 450: tol = 0.014794475998481382\n",
      "iters 500: tol = 0.0313235008391603\n",
      "iters 550: tol = 0.00967627246806857\n",
      "iters 600: tol = 0.001663214982941641\n",
      "iters 650: tol = 0.000247564897058572\n",
      "iters 50: tol = 0.008742818270767394\n",
      "iters 100: tol = 0.004452360871662009\n",
      "iters 150: tol = 0.002217697089484716\n",
      "iters 200: tol = 0.0010380277106506464\n",
      "iters 250: tol = 0.0004756290832303445\n",
      "iters 300: tol = 0.00021629936449246712\n",
      "iters 50: tol = 0.014042135596038063\n",
      "iters 100: tol = 0.012475424127992962\n",
      "iters 150: tol = 0.007555617161090433\n",
      "iters 200: tol = 0.0032861207825556904\n",
      "iters 250: tol = 0.0017858587102645984\n",
      "iters 300: tol = 0.0009330636149529337\n",
      "iters 350: tol = 0.0004150336388135667\n",
      "iters 400: tol = 0.00017609292202580562\n",
      "iters 50: tol = 0.012226493134589678\n",
      "iters 100: tol = 0.008045477862600348\n",
      "iters 150: tol = 0.004540119520415775\n",
      "iters 200: tol = 0.002244650876745524\n",
      "iters 250: tol = 0.0010442840346265303\n",
      "iters 300: tol = 0.00047445458575423594\n",
      "iters 350: tol = 0.00021356800832883494\n",
      "iters 50: tol = 0.02081236818490506\n",
      "iters 100: tol = 0.005978064715516096\n",
      "iters 150: tol = 0.0018056850604175612\n",
      "iters 200: tol = 0.0005633388021286656\n",
      "iters 250: tol = 0.0001779231054139796\n",
      "iters 50: tol = 0.008043289553272409\n",
      "iters 100: tol = 0.007828462609907896\n",
      "iters 150: tol = 0.011055874574236402\n",
      "iters 200: tol = 0.019801997230406654\n",
      "iters 250: tol = 0.0251307521754196\n",
      "iters 300: tol = 0.036398942341965446\n",
      "iters 350: tol = 0.01665463185328675\n",
      "iters 400: tol = 0.01345505511076217\n",
      "iters 450: tol = 0.0030621818139406898\n",
      "iters 500: tol = 0.0015276900279674877\n",
      "iters 550: tol = 0.0006613797381429531\n",
      "iters 600: tol = 0.0002918579267078836\n",
      "iters 650: tol = 0.0001313576040827824\n",
      "iters 50: tol = 0.005391860527068859\n",
      "iters 100: tol = 0.001392567371205411\n",
      "iters 150: tol = 0.0002992795379963553\n",
      "2005-12-01 00:00:00 is done and took 184 iterations and 117.95 seconds\n",
      "iters 50: tol = 0.0025603109704039895\n",
      "iters 50: tol = 0.00043102668997385685\n",
      "iters 50: tol = 0.00021392363412992\n",
      "iters 50: tol = 0.00030586135624399713\n",
      "iters 50: tol = 0.004326902194147664\n",
      "iters 100: tol = 0.0001753651580020943\n",
      "iters 50: tol = 0.0011572411639441949\n",
      "iters 50: tol = 0.0008765052597831113\n",
      "iters 50: tol = 0.0009453161674954913\n",
      "iters 100: tol = 0.00011773223382149922\n",
      "iters 50: tol = 0.0011861553194061347\n",
      "iters 100: tol = 0.00010241034738123211\n",
      "iters 50: tol = 0.0004062479112002526\n",
      "iters 50: tol = 0.00036771458717488326\n",
      "iters 50: tol = 0.0002418886583741564\n",
      "2006-12-01 00:00:00 is done and took 62 iterations and 97.92 seconds\n",
      "iters 50: tol = 0.0011236851669783193\n",
      "iters 50: tol = 0.0472939969569012\n",
      "iters 100: tol = 0.08608960962903517\n",
      "iters 150: tol = 0.013560803482594877\n",
      "iters 200: tol = 0.0020189420776883515\n",
      "iters 250: tol = 0.0001569284039418528\n",
      "iters 50: tol = 0.0011491798789286037\n",
      "iters 50: tol = 0.04350271888230471\n",
      "iters 100: tol = 6.6450627228972765\n",
      "iters 150: tol = 0.01109841740338302\n",
      "iters 200: tol = 0.0005776222934330555\n",
      "iters 50: tol = 0.00027956842995946474\n",
      "iters 50: tol = 0.00039355123595741226\n",
      "iters 50: tol = 0.00034340303266622296\n",
      "iters 50: tol = 0.0011774705069946823\n",
      "iters 100: tol = 0.00011452305489378922\n",
      "iters 50: tol = 0.0003927300610615525\n",
      "iters 50: tol = 0.00026908390134461335\n",
      "iters 50: tol = 0.00014645080108177666\n",
      "iters 50: tol = 0.000835818937491295\n",
      "2007-12-01 00:00:00 is done and took 86 iterations and 134.00 seconds\n",
      "iters 50: tol = 0.0014205900847987785\n",
      "iters 100: tol = 0.00017125232290493564\n",
      "iters 50: tol = 0.0016995184156075593\n",
      "iters 100: tol = 0.0004865279144332657\n",
      "iters 150: tol = 0.00015311835640585691\n",
      "iters 50: tol = 0.0020723270541112004\n",
      "iters 100: tol = 0.00030831473768289097\n",
      "iters 50: tol = 0.0011755318708370766\n",
      "iters 100: tol = 0.0002505990760547361\n",
      "iters 50: tol = 0.0012796946043172053\n",
      "iters 100: tol = 0.00032929909233914145\n",
      "iters 50: tol = 0.031767382666843424\n",
      "iters 100: tol = 0.008057774253608874\n",
      "iters 150: tol = 0.001732238429586308\n",
      "iters 200: tol = 0.00035828569741003236\n",
      "iters 50: tol = 0.011654571660645985\n",
      "iters 100: tol = 0.00721451118422245\n",
      "iters 150: tol = 0.007628318641984211\n",
      "iters 200: tol = 0.014932114028965882\n",
      "iters 250: tol = 0.520459727156283\n",
      "iters 300: tol = 0.009437288493640228\n",
      "iters 350: tol = 0.002706031846161694\n",
      "iters 400: tol = 0.0006565667398116259\n",
      "iters 450: tol = 0.00016526816681439183\n",
      "iters 50: tol = 0.003523861552909846\n",
      "iters 100: tol = 0.0007353283802163102\n",
      "iters 150: tol = 0.0001751604667493134\n",
      "iters 50: tol = 0.0026788378151290093\n",
      "iters 100: tol = 0.0006406824199495231\n",
      "iters 150: tol = 0.00016680637061150527\n",
      "iters 50: tol = 0.0007603433311719754\n",
      "iters 100: tol = 0.00022085712656688683\n",
      "iters 50: tol = 0.00152121879635847\n",
      "iters 100: tol = 0.0003365472847600137\n",
      "iters 150: tol = 0.00021876488670491412\n",
      "iters 200: tol = 0.00013847622333851284\n",
      "iters 50: tol = 0.003647159977978487\n",
      "iters 100: tol = 0.0014104538102525654\n",
      "iters 150: tol = 0.0007996127067449454\n",
      "iters 200: tol = 0.0004617569647039077\n",
      "iters 250: tol = 0.00026133087021631973\n",
      "iters 300: tol = 0.00014566210816122083\n",
      "2008-12-01 00:00:00 is done and took 332 iterations and 428.98 seconds\n",
      "iters 50: tol = 0.0035564163525838577\n",
      "iters 100: tol = 0.0012627921615729898\n",
      "iters 150: tol = 0.00044579733731087146\n",
      "iters 200: tol = 0.00017227174358613873\n",
      "iters 50: tol = 0.0003472747783790364\n",
      "iters 50: tol = 0.00042640301730068053\n",
      "iters 100: tol = 0.000248289935256496\n",
      "iters 150: tol = 0.0001424188078753552\n",
      "iters 50: tol = 0.005329805388767372\n",
      "iters 100: tol = 0.003996703767551291\n",
      "iters 150: tol = 0.0026451622718364487\n",
      "iters 200: tol = 0.0015211112062306423\n",
      "iters 250: tol = 0.000872391982986942\n",
      "iters 300: tol = 0.0004643654041083245\n",
      "iters 350: tol = 0.0002336621270953021\n",
      "iters 400: tol = 0.00011437023150989711\n",
      "iters 50: tol = 0.008627357608397723\n",
      "iters 100: tol = 0.004070821461870988\n",
      "iters 150: tol = 0.002046466562225513\n",
      "iters 200: tol = 0.0011421643706880369\n",
      "iters 250: tol = 0.0006806033802451705\n",
      "iters 300: tol = 0.0004129565170855609\n",
      "iters 350: tol = 0.00025353211835021927\n",
      "iters 400: tol = 0.00015682185882831545\n",
      "iters 50: tol = 0.015751090705429106\n",
      "iters 100: tol = 0.00576210628001661\n",
      "iters 150: tol = 0.001023280471978305\n",
      "iters 200: tol = 0.00016511913479755336\n",
      "iters 50: tol = 0.002539130340517892\n",
      "iters 100: tol = 0.0003484546155231305\n",
      "iters 150: tol = 0.00011885317432969167\n",
      "iters 50: tol = 0.16007247632166433\n",
      "iters 100: tol = 0.011130771168362363\n",
      "iters 150: tol = 0.004814366841401574\n",
      "iters 200: tol = 0.0018697697678185174\n",
      "iters 250: tol = 0.0006596838998498555\n",
      "iters 300: tol = 0.0002243376620869736\n",
      "iters 50: tol = 0.014771301361441225\n",
      "iters 100: tol = 0.0008098536394756106\n",
      "iters 150: tol = 0.00018332487273220455\n",
      "iters 50: tol = 0.0025724137145215487\n",
      "iters 100: tol = 0.0007001726712460021\n",
      "iters 150: tol = 0.00018628418130384183\n",
      "iters 50: tol = 0.005094464423163747\n",
      "iters 100: tol = 0.0025752047114669663\n",
      "iters 150: tol = 0.0014129697563207422\n",
      "iters 200: tol = 0.0008218725014282058\n",
      "iters 250: tol = 0.0004953220767181937\n",
      "iters 300: tol = 0.00030496978018961646\n",
      "iters 350: tol = 0.000190230197314116\n",
      "iters 400: tol = 0.00011961604805124995\n",
      "iters 50: tol = 0.0023833916003226374\n",
      "iters 100: tol = 0.0012442297090929921\n",
      "iters 150: tol = 0.0008835406299055881\n",
      "iters 200: tol = 0.0006157719012429563\n",
      "iters 250: tol = 0.0004321958080975574\n",
      "iters 300: tol = 0.0003057749049897862\n",
      "iters 350: tol = 0.00021762057961094428\n",
      "iters 400: tol = 0.0001555299017654277\n",
      "iters 450: tol = 0.00011149539583548557\n",
      "2009-12-01 00:00:00 is done and took 467 iterations and 366.65 seconds\n",
      "iters 50: tol = 0.0027512639867048883\n",
      "iters 100: tol = 0.0013176963893680727\n",
      "iters 150: tol = 0.0005801599830603243\n",
      "iters 200: tol = 0.00024697529108780314\n",
      "iters 250: tol = 0.00010365107117527028\n",
      "iters 50: tol = 0.0009113030293024238\n",
      "iters 100: tol = 0.00032829547212359644\n",
      "iters 150: tol = 0.00011753905293804268\n",
      "iters 50: tol = 0.0006089800022701652\n",
      "iters 100: tol = 0.00021737714298675215\n",
      "iters 50: tol = 0.012156149879989975\n",
      "iters 100: tol = 0.006897108680361308\n",
      "iters 150: tol = 0.003645516469479526\n",
      "iters 200: tol = 0.0021787208604655967\n",
      "iters 250: tol = 0.001393463516934923\n",
      "iters 300: tol = 0.0009297833513699227\n",
      "iters 350: tol = 0.0006426617062604834\n",
      "iters 400: tol = 0.0004552036076479471\n",
      "iters 450: tol = 0.00032722694036246835\n",
      "iters 500: tol = 0.00023803416947798528\n",
      "iters 550: tol = 0.0001749435606192007\n",
      "iters 600: tol = 0.00012927653246952442\n",
      "iters 50: tol = 0.0032082388563527964\n",
      "iters 100: tol = 0.001975634122764802\n",
      "iters 150: tol = 0.0013263310080575685\n",
      "iters 200: tol = 0.0009567201345452303\n",
      "iters 250: tol = 0.0008095303350267868\n",
      "iters 300: tol = 0.0006600361817696576\n",
      "iters 350: tol = 0.0005144827695452991\n",
      "iters 400: tol = 0.0003844222132469155\n",
      "iters 450: tol = 0.0002773923659941946\n",
      "iters 500: tol = 0.00019490678048494914\n",
      "iters 550: tol = 0.00013433157292649933\n",
      "iters 50: tol = 0.05496632203768326\n",
      "iters 100: tol = 0.003309668365617302\n",
      "iters 150: tol = 0.0006559089254184469\n",
      "iters 200: tol = 0.0001668765872616662\n",
      "iters 50: tol = 0.001132593171463192\n",
      "iters 100: tol = 0.0001010667065679538\n",
      "iters 50: tol = 0.007768526875881435\n",
      "iters 100: tol = 0.003308190050544002\n",
      "iters 150: tol = 0.001767046650153703\n",
      "iters 200: tol = 0.0011471534591339372\n",
      "iters 250: tol = 0.0008188321044504399\n",
      "iters 300: tol = 0.0006159306227393191\n",
      "iters 350: tol = 0.0004801161544393667\n",
      "iters 400: tol = 0.0003837118762370223\n",
      "iters 450: tol = 0.0003122112331015933\n",
      "iters 500: tol = 0.00025738758576154125\n",
      "iters 550: tol = 0.0002142687618383876\n",
      "iters 600: tol = 0.00017968630142152758\n",
      "iters 650: tol = 0.00015152832136879253\n",
      "iters 700: tol = 0.00012833147218062856\n",
      "iters 750: tol = 0.00010904695145608906\n",
      "iters 50: tol = 0.026125273031309626\n",
      "iters 100: tol = 0.0015352052315886766\n",
      "iters 150: tol = 0.0002801356406697958\n",
      "iters 50: tol = 0.0024509559229475286\n",
      "iters 100: tol = 0.0012896501439334518\n",
      "iters 150: tol = 0.0009431488602604787\n",
      "iters 200: tol = 0.0006871199947664497\n",
      "iters 250: tol = 0.00046933870378029763\n",
      "iters 300: tol = 0.00030845156467007584\n",
      "iters 350: tol = 0.0001977872095918487\n",
      "iters 400: tol = 0.0001248537950023776\n",
      "iters 50: tol = 0.002603875701885361\n",
      "iters 100: tol = 0.0012046053862494466\n",
      "iters 150: tol = 0.0006547814515844783\n",
      "iters 200: tol = 0.0003624241821903948\n",
      "iters 250: tol = 0.00019784953116622553\n",
      "iters 300: tol = 0.0001075281914205764\n",
      "iters 50: tol = 0.0011462374758580349\n",
      "iters 100: tol = 0.0006362830811129511\n",
      "iters 150: tol = 0.00037672738777722037\n",
      "iters 200: tol = 0.0002290827314081617\n",
      "iters 250: tol = 0.00014066908116784627\n",
      "2010-12-01 00:00:00 is done and took 286 iterations and 197.69 seconds\n",
      "iters 50: tol = 0.001088536657601058\n",
      "iters 100: tol = 0.0007582481488867254\n",
      "iters 150: tol = 0.0005708186484909006\n",
      "iters 200: tol = 0.0004387321327637972\n",
      "iters 250: tol = 0.00034084390090410865\n",
      "iters 300: tol = 0.0002667786095704727\n",
      "iters 350: tol = 0.00020997846163262057\n",
      "iters 400: tol = 0.00016598162418701712\n",
      "iters 450: tol = 0.0001316399723185202\n",
      "iters 500: tol = 0.0001046746569395296\n",
      "iters 50: tol = 0.0021788817488767043\n",
      "iters 100: tol = 0.0016630578110866656\n",
      "iters 150: tol = 0.0009900239400274513\n",
      "iters 200: tol = 0.0005659295783176921\n",
      "iters 250: tol = 0.0003210542335065625\n",
      "iters 300: tol = 0.00018133142712076222\n",
      "iters 350: tol = 0.00010182486737050911\n",
      "iters 50: tol = 0.0001802657934935059\n",
      "iters 50: tol = 0.0023613395644357174\n",
      "iters 100: tol = 0.0009260806334311471\n",
      "iters 150: tol = 0.00042861166645975085\n",
      "iters 200: tol = 0.00021052856266737252\n",
      "iters 250: tol = 0.00010552465007772349\n",
      "iters 50: tol = 0.018280348580321015\n",
      "iters 100: tol = 0.007485493081895056\n",
      "iters 150: tol = 0.004897247113649916\n",
      "iters 200: tol = 0.004057682286329994\n",
      "iters 250: tol = 0.004046610744604149\n",
      "iters 300: tol = 0.0046034453745502635\n",
      "iters 350: tol = 0.0056537895242455005\n",
      "iters 400: tol = 0.007037764979917149\n",
      "iters 450: tol = 0.008078802688395115\n",
      "iters 500: tol = 0.008019019894372481\n",
      "iters 550: tol = 0.006489452012528041\n",
      "iters 600: tol = 0.005034610590141098\n",
      "iters 650: tol = 0.005321884908737418\n",
      "iters 700: tol = 0.0066926214552150975\n",
      "iters 750: tol = 0.013456812103794569\n",
      "iters 800: tol = 0.05061846695381256\n",
      "iters 850: tol = 0.025542287459104696\n",
      "iters 900: tol = 0.005177704056390509\n",
      "iters 950: tol = 0.0015302554485492337\n",
      "iters 1000: tol = 0.0005667276007690347\n",
      "iters 1050: tol = 0.00021148838407913928\n",
      "iters 50: tol = 0.0017174019333520696\n",
      "iters 100: tol = 0.0002609935494476412\n",
      "iters 50: tol = 0.0012111846366229528\n",
      "iters 100: tol = 0.00026652528663762\n",
      "iters 50: tol = 0.003055544200731175\n",
      "iters 100: tol = 0.000782312290361159\n",
      "iters 150: tol = 0.00022415213073995188\n",
      "iters 50: tol = 0.0021439591605791897\n",
      "iters 100: tol = 5.766650562948763\n",
      "iters 150: tol = 0.00012363080076860378\n",
      "iters 50: tol = 0.0031938729708730906\n",
      "iters 100: tol = 0.0005432090658384348\n",
      "iters 50: tol = 0.0022320340763926083\n",
      "iters 100: tol = 9.50191471391637e-05\n",
      "iters 50: tol = 0.00011711960950211431\n",
      "2011-12-01 00:00:00 is done and took 53 iterations and 88.29 seconds\n",
      "iters 50: tol = 0.000364314489019725\n",
      "iters 50: tol = 0.0006902183866510647\n",
      "iters 50: tol = 0.0007806199453250784\n",
      "iters 100: tol = 9.769882389165652e-05\n",
      "iters 50: tol = 0.0011409028944363575\n",
      "iters 100: tol = 0.00010655223668765146\n",
      "iters 50: tol = 0.0003021785063632709\n",
      "iters 50: tol = 0.00030251295978905857\n",
      "iters 50: tol = 0.0007794294636666588\n",
      "iters 100: tol = 0.0001442395167027266\n",
      "iters 50: tol = 0.0034777875860947383\n",
      "iters 100: tol = 0.001192233374763485\n",
      "iters 150: tol = 0.0005065714266742072\n",
      "iters 200: tol = 0.00022644282113470915\n",
      "iters 250: tol = 0.0001021968725481992\n",
      "iters 50: tol = 0.0022815574759592616\n",
      "iters 100: tol = 0.001207690061122424\n",
      "iters 150: tol = 0.0006322276861963072\n",
      "iters 200: tol = 0.00033933180907635974\n",
      "iters 250: tol = 0.0001789973055281724\n",
      "iters 50: tol = 0.0015739393803911872\n",
      "iters 100: tol = 0.0003866676918866663\n",
      "iters 150: tol = 0.0003101789349639894\n",
      "iters 200: tol = 0.00018680300807599748\n",
      "iters 250: tol = 0.00010765235197024436\n",
      "iters 50: tol = 0.008492976663015628\n",
      "iters 100: tol = 0.01758964205775082\n",
      "iters 150: tol = 0.011503298061352668\n",
      "iters 200: tol = 0.004415456706400311\n",
      "iters 250: tol = 0.0017519073990447387\n",
      "iters 300: tol = 0.0007442654033992802\n",
      "iters 350: tol = 0.00032824779424356354\n",
      "iters 400: tol = 0.0001473842914773149\n",
      "iters 50: tol = 0.012355604101175026\n",
      "iters 100: tol = 0.007051615457826621\n",
      "iters 150: tol = 0.0028029737437870184\n",
      "iters 200: tol = 0.001108700812392005\n",
      "iters 250: tol = 0.00044958932717720224\n",
      "iters 300: tol = 0.0001848617923193574\n",
      "2012-12-01 00:00:00 is done and took 335 iterations and 247.38 seconds\n",
      "iters 50: tol = 0.011843310430691811\n",
      "iters 100: tol = 0.0046536060737027984\n",
      "iters 150: tol = 0.001950687217435798\n",
      "iters 200: tol = 0.0009180463924111371\n",
      "iters 250: tol = 0.00046356221716215007\n",
      "iters 300: tol = 0.00024331824207921016\n",
      "iters 350: tol = 0.0001304278478893306\n",
      "iters 50: tol = 0.017475306239433053\n",
      "iters 100: tol = 0.017258442698964677\n",
      "iters 150: tol = 0.005854032321684299\n",
      "iters 200: tol = 0.001761800430151117\n",
      "iters 250: tol = 0.0007034783592798632\n",
      "iters 300: tol = 0.0003645583864333446\n",
      "iters 350: tol = 0.00017494849685373293\n",
      "iters 50: tol = 0.007345366053783164\n",
      "iters 100: tol = 0.003851870361081644\n",
      "iters 150: tol = 0.0018092964198908844\n",
      "iters 200: tol = 0.0008190003893817979\n",
      "iters 250: tol = 0.0003676067021600171\n",
      "iters 300: tol = 0.00016469936173911515\n",
      "iters 50: tol = 0.006801486405745183\n",
      "iters 100: tol = 0.002623579356317851\n",
      "iters 150: tol = 0.001202950227093158\n",
      "iters 200: tol = 0.0005280954641545543\n",
      "iters 250: tol = 0.0002290012733995317\n",
      "iters 300: tol = 9.89769928112505e-05\n",
      "iters 50: tol = 0.0828592679878397\n",
      "iters 100: tol = 0.005098501373788533\n",
      "iters 150: tol = 0.002623857222726844\n",
      "iters 200: tol = 0.0012038649432726256\n",
      "iters 250: tol = 0.0005130743350422184\n",
      "iters 300: tol = 0.00021181106933365612\n",
      "iters 50: tol = 0.003988834959512633\n",
      "iters 100: tol = 0.0017447274959509484\n",
      "iters 150: tol = 0.0006022111740693892\n",
      "iters 200: tol = 0.00019174971313073996\n",
      "iters 50: tol = 0.0082344664590418\n",
      "iters 100: tol = 0.0018173708324161764\n",
      "iters 150: tol = 0.00042724863582788153\n",
      "iters 200: tol = 0.00011313984727978621\n",
      "iters 50: tol = 0.003808788558909426\n",
      "iters 100: tol = 0.004647013338053174\n",
      "iters 150: tol = 0.01929366931841732\n",
      "iters 200: tol = 0.043512868923712984\n",
      "iters 250: tol = 0.0045491752393916896\n",
      "iters 300: tol = 0.0002984428972205855\n",
      "iters 50: tol = 0.008793423694566621\n",
      "iters 100: tol = 0.00029107766302460814\n",
      "iters 50: tol = 0.0009592618164171451\n",
      "iters 50: tol = 0.0003821821265097558\n",
      "iters 50: tol = 0.0008027831181999234\n",
      "2013-12-01 00:00:00 is done and took 86 iterations and 200.49 seconds\n",
      "iters 50: tol = 0.001239345637938316\n",
      "iters 100: tol = 0.0001555725301749522\n",
      "iters 50: tol = 0.00280250332671525\n",
      "iters 100: tol = 0.00046353104410333523\n",
      "iters 50: tol = 0.00044842626329044677\n",
      "iters 50: tol = 0.001222288721412168\n",
      "iters 100: tol = 0.00014540810343377508\n",
      "iters 50: tol = 0.0005586156872599002\n",
      "iters 50: tol = 0.0007083450112890244\n",
      "iters 100: tol = 0.00012449700800831742\n",
      "iters 50: tol = 0.005923954886400906\n",
      "iters 100: tol = 0.006989767424114168\n",
      "iters 150: tol = 0.0077641297496439665\n",
      "iters 200: tol = 0.002500321429574376\n",
      "iters 250: tol = 0.0007524156032588936\n",
      "iters 300: tol = 0.0002711665194797819\n",
      "iters 50: tol = 0.0017022444213892207\n",
      "iters 100: tol = 0.0002902995972855482\n",
      "iters 50: tol = 0.004703638999333959\n",
      "iters 100: tol = 0.0008950741838383847\n",
      "iters 150: tol = 0.00013203284328422438\n",
      "iters 50: tol = 0.0005166083093045559\n",
      "iters 50: tol = 0.0006343365382137645\n",
      "iters 50: tol = 0.004533910501173133\n",
      "iters 100: tol = 0.0006170149539842518\n",
      "2014-12-01 00:00:00 is done and took 143 iterations and 115.97 seconds\n",
      "iters 50: tol = 0.005760977217817098\n",
      "iters 100: tol = 0.0008002337437253981\n",
      "iters 50: tol = 0.0028412429767890046\n",
      "iters 100: tol = 0.0002497131054268742\n",
      "iters 50: tol = 0.020428389323409443\n",
      "iters 100: tol = 0.0012076236516166405\n",
      "iters 150: tol = 0.00010931417633308627\n",
      "iters 50: tol = 0.00770779062519622\n",
      "iters 100: tol = 0.00026817972332970896\n",
      "iters 50: tol = 0.0033951966489638163\n",
      "iters 100: tol = 0.0002387636033669427\n",
      "iters 50: tol = 0.0013013345162262713\n",
      "iters 100: tol = 0.00012459119756980108\n",
      "iters 50: tol = 0.006648702494983505\n",
      "iters 100: tol = 0.0017859989125743603\n",
      "iters 150: tol = 0.0011278570740245186\n",
      "iters 200: tol = 0.0007135472715531499\n",
      "iters 250: tol = 0.00045186696819676797\n",
      "iters 300: tol = 0.0002862618015160301\n",
      "iters 350: tol = 0.00018139545204620688\n",
      "iters 400: tol = 0.00011497060379284108\n",
      "iters 50: tol = 0.008941642114154824\n",
      "iters 100: tol = 0.003034990033568885\n",
      "iters 150: tol = 0.0016777529004948222\n",
      "iters 200: tol = 0.0022091279088068405\n",
      "iters 250: tol = 0.003830393908173635\n",
      "iters 300: tol = 0.004411233520596247\n",
      "iters 350: tol = 0.0022842124932158647\n",
      "iters 400: tol = 0.0007628762188582883\n",
      "iters 450: tol = 0.000254784480674064\n",
      "iters 50: tol = 0.0010556776169230453\n",
      "iters 100: tol = 0.00023032567756986477\n",
      "iters 50: tol = 0.0009915792424278758\n",
      "iters 100: tol = 0.00014052628344768392\n",
      "iters 50: tol = 0.000899653794708577\n",
      "iters 100: tol = 0.0001285801836277134\n",
      "iters 50: tol = 0.002454452976513505\n",
      "iters 100: tol = 0.00045210104422738207\n",
      "iters 150: tol = 9.983713265304672e-05\n",
      "2015-12-01 00:00:00 is done and took 150 iterations and 133.60 seconds\n",
      "iters 50: tol = 0.00157214229969882\n",
      "iters 100: tol = 0.0006327632645497516\n",
      "iters 150: tol = 0.0002906378893765549\n",
      "iters 200: tol = 0.00013401559021364307\n",
      "iters 50: tol = 0.007130314196635235\n",
      "iters 100: tol = 0.002762661343162387\n",
      "iters 150: tol = 0.0012235932909132607\n",
      "iters 200: tol = 0.0006228665993295301\n",
      "iters 250: tol = 0.0003025197705908145\n",
      "iters 300: tol = 0.00014375404907740474\n",
      "iters 50: tol = 0.0009646197204643547\n",
      "iters 100: tol = 0.0006112063480219337\n",
      "iters 150: tol = 0.00031746262173328044\n",
      "iters 200: tol = 0.00015351148574629936\n",
      "iters 50: tol = 0.005286709430038894\n",
      "iters 100: tol = 0.0024213154038772444\n",
      "iters 150: tol = 0.001407366826746248\n",
      "iters 200: tol = 0.0008917574284224017\n",
      "iters 250: tol = 0.0006495776196867586\n",
      "iters 300: tol = 0.0006208708517396744\n",
      "iters 350: tol = 0.0007511641272830438\n",
      "iters 400: tol = 0.0011392194780250886\n",
      "iters 450: tol = 0.0020429619037252156\n",
      "iters 500: tol = 0.003953372987981485\n",
      "iters 550: tol = 0.006169939914870762\n",
      "iters 600: tol = 0.004419987728323083\n",
      "iters 650: tol = 0.0063330351218476855\n",
      "iters 700: tol = 0.011327376203480344\n",
      "iters 750: tol = 0.009099573851782616\n",
      "iters 800: tol = 0.013863401536633796\n",
      "iters 850: tol = 0.005831004179173047\n",
      "iters 900: tol = 0.008758340693076017\n",
      "iters 950: tol = 0.03436913399165912\n",
      "iters 1000: tol = 0.025867682880682474\n",
      "iters 1050: tol = 0.00267858472774668\n",
      "iters 1100: tol = 0.00020218583887512964\n",
      "iters 50: tol = 0.0054450716011767986\n",
      "iters 100: tol = 0.0015075523692409387\n",
      "iters 150: tol = 0.0004405204677554453\n",
      "iters 200: tol = 0.00013078917218101171\n",
      "iters 50: tol = 0.004899291059884181\n",
      "iters 100: tol = 0.0020372793957341706\n",
      "iters 150: tol = 0.0009072361065518564\n",
      "iters 200: tol = 0.0004131105433571314\n",
      "iters 250: tol = 0.0001899929589650462\n",
      "iters 50: tol = 0.00478595688213912\n",
      "iters 100: tol = 0.0016533946937968835\n",
      "iters 150: tol = 0.0005820971086044624\n",
      "iters 200: tol = 0.00020031288789412738\n",
      "iters 50: tol = 0.007363132410416917\n",
      "iters 100: tol = 0.001890934594811322\n",
      "iters 150: tol = 0.000581752332664609\n",
      "iters 200: tol = 0.0002538795824408302\n",
      "iters 250: tol = 0.00012115970913395557\n",
      "iters 50: tol = 0.0022980794263450233\n",
      "iters 100: tol = 0.0010343513875842314\n",
      "iters 150: tol = 0.0008300219872079406\n",
      "iters 200: tol = 0.0006517610974199339\n",
      "iters 250: tol = 0.0005682993598437047\n",
      "iters 300: tol = 0.0006750523917701257\n",
      "iters 350: tol = 0.0008313963263252844\n",
      "iters 400: tol = 0.0010494376822135498\n",
      "iters 450: tol = 0.0013531704285795199\n",
      "iters 500: tol = 0.0017915798553187479\n",
      "iters 550: tol = 0.0024400604117931213\n",
      "iters 600: tol = 0.003393960384537431\n",
      "iters 650: tol = 0.004667855493382889\n",
      "iters 700: tol = 0.0064490355288353285\n",
      "iters 750: tol = 0.0073128761282835325\n",
      "iters 800: tol = 0.006219450422608408\n",
      "iters 850: tol = 0.004588162105875648\n",
      "iters 900: tol = 0.003331412182742133\n",
      "iters 950: tol = 0.0024033801428912094\n",
      "iters 1000: tol = 0.0017747959986376127\n",
      "iters 1050: tol = 0.0013513968057060621\n",
      "iters 1100: tol = 0.0010602339430370589\n",
      "iters 1150: tol = 0.0008544793932164785\n",
      "iters 1200: tol = 0.0007051074751256436\n",
      "iters 1250: tol = 0.0005939823810204858\n",
      "iters 1300: tol = 0.0005095221638795056\n",
      "iters 1350: tol = 0.00044413892472866534\n",
      "iters 1400: tol = 0.0003927330264105189\n",
      "iters 1450: tol = 0.000351794784055115\n",
      "iters 1500: tol = 0.00031885598892891776\n",
      "iters 1550: tol = 0.00029214721754328155\n",
      "iters 1600: tol = 0.00027173689966927816\n",
      "iters 1650: tol = 0.0002556180132729513\n",
      "iters 1700: tol = 0.00024283322072463087\n",
      "iters 1750: tol = 0.00023290969299097353\n",
      "iters 1800: tol = 0.0002255153697455492\n",
      "iters 1850: tol = 0.00022217856338568875\n",
      "iters 1900: tol = 0.00022472554017871627\n",
      "iters 1950: tol = 0.00023060792628554205\n",
      "iters 2000: tol = 0.00024341926849325013\n",
      "iters 2050: tol = 0.00026549954536791986\n",
      "iters 2100: tol = 0.00032341215864374606\n",
      "iters 2150: tol = 0.00040602916246876286\n",
      "iters 2200: tol = 0.0005289177697967162\n",
      "iters 2250: tol = 0.000721694817088607\n",
      "iters 2300: tol = 0.00104407563764998\n",
      "iters 2350: tol = 0.0016207350836073692\n",
      "iters 2400: tol = 0.002676114929334561\n",
      "iters 2450: tol = 0.004129180599258764\n",
      "iters 2500: tol = 0.005512305659902461\n",
      "iters 2550: tol = 0.005145595572243811\n",
      "iters 2600: tol = 0.003043662632419092\n",
      "iters 2650: tol = 0.0019559319166775613\n",
      "iters 2700: tol = 0.00132465415518912\n",
      "iters 2750: tol = 0.0009210237789615272\n",
      "iters 2800: tol = 0.0006343626007661585\n",
      "iters 2850: tol = 0.0004341298428375828\n",
      "iters 2900: tol = 0.0002957378032629743\n",
      "iters 2950: tol = 0.00020081580110675734\n",
      "iters 3000: tol = 0.0001360596111332213\n",
      "iters 50: tol = 0.007777526293120007\n",
      "iters 100: tol = 0.0029662608397644785\n",
      "iters 150: tol = 0.0011421186411171291\n",
      "iters 200: tol = 0.0004360574042631127\n",
      "iters 250: tol = 0.00016746660587274587\n",
      "iters 50: tol = 0.001502173177856836\n",
      "iters 100: tol = 0.00033642301866487756\n",
      "iters 150: tol = 0.00014638339961725588\n",
      "iters 50: tol = 0.0012893907076660938\n",
      "iters 100: tol = 0.00026103089309037486\n",
      "iters 150: tol = 0.00011907562894863943\n",
      "2016-12-01 00:00:00 is done and took 162 iterations and 166.82 seconds\n",
      "iters 50: tol = 0.0016879056719663055\n",
      "iters 100: tol = 0.00042744182883749926\n",
      "iters 150: tol = 0.000148069500182757\n",
      "iters 50: tol = 0.003593220305317363\n",
      "iters 100: tol = 0.0021938626304155717\n",
      "iters 150: tol = 0.0018701195771290702\n",
      "iters 200: tol = 0.0018800102306940625\n",
      "iters 250: tol = 0.0020968277764769477\n",
      "iters 300: tol = 0.0024252831324027696\n",
      "iters 350: tol = 0.0025265660818580005\n",
      "iters 400: tol = 0.002306966234150387\n",
      "iters 450: tol = 0.0018308649973824975\n",
      "iters 500: tol = 0.0015584471338092598\n",
      "iters 550: tol = 0.001586115057642598\n",
      "iters 600: tol = 0.0012439872315590378\n",
      "iters 650: tol = 0.0008031249396102114\n",
      "iters 700: tol = 0.00046571305752662884\n",
      "iters 750: tol = 0.00025602877823691084\n",
      "iters 800: tol = 0.00013725959014390665\n",
      "iters 50: tol = 0.00484743743720184\n",
      "iters 100: tol = 0.004323477821914445\n",
      "iters 150: tol = 0.0026223076217525287\n",
      "iters 200: tol = 0.0015250525567830908\n",
      "iters 250: tol = 0.0009194128225233356\n",
      "iters 300: tol = 0.0005779578495304083\n",
      "iters 350: tol = 0.00037580487848948296\n",
      "iters 400: tol = 0.00025048152320145123\n",
      "iters 450: tol = 0.00016987682843977447\n",
      "iters 500: tol = 0.00011659683499826157\n",
      "iters 50: tol = 0.0021642684816865487\n",
      "iters 100: tol = 0.0007114117588888913\n",
      "iters 150: tol = 0.000318680681285044\n",
      "iters 200: tol = 0.00014791066223085458\n",
      "iters 50: tol = 0.0018177129353931232\n",
      "iters 100: tol = 0.0012283265364137763\n",
      "iters 150: tol = 0.0007975002038160678\n",
      "iters 200: tol = 0.000501859518169005\n",
      "iters 250: tol = 0.0003101088058309921\n",
      "iters 300: tol = 0.00018974720705258047\n",
      "iters 350: tol = 0.00011550895685120965\n",
      "iters 50: tol = 0.0018098997649805826\n",
      "iters 100: tol = 0.0005986186179306463\n",
      "iters 150: tol = 0.00033610786214403887\n",
      "iters 200: tol = 0.00020663793160292931\n",
      "iters 250: tol = 0.00012408889626826236\n",
      "iters 50: tol = 0.003954877325055117\n",
      "iters 100: tol = 0.003244923845368841\n",
      "iters 150: tol = 0.0023992619832624573\n",
      "iters 200: tol = 0.0016274780578352036\n",
      "iters 250: tol = 0.0010698536582911672\n",
      "iters 300: tol = 0.0006917450328092142\n",
      "iters 350: tol = 0.0004452831343118649\n",
      "iters 400: tol = 0.000287279056430334\n",
      "iters 450: tol = 0.00018501342140864185\n",
      "iters 500: tol = 0.00011895450002696872\n",
      "iters 50: tol = 0.002948682052085816\n",
      "iters 100: tol = 0.0010479840355147596\n",
      "iters 150: tol = 0.0005197078833495405\n",
      "iters 200: tol = 0.0002613500879132258\n",
      "iters 250: tol = 0.00013091922367602926\n",
      "iters 50: tol = 0.004192679265345944\n",
      "iters 100: tol = 0.002368565883108742\n",
      "iters 150: tol = 0.0013433848103269752\n",
      "iters 200: tol = 0.0007460385340710418\n",
      "iters 250: tol = 0.0004119168601295631\n",
      "iters 300: tol = 0.00022723791483392208\n",
      "iters 350: tol = 0.0001253948675068728\n",
      "iters 50: tol = 0.0038321066771117296\n",
      "iters 100: tol = 0.0015623246809713387\n",
      "iters 150: tol = 0.0006018888090268404\n",
      "iters 200: tol = 0.00022858063874098278\n",
      "iters 50: tol = 0.0015534118760924809\n",
      "iters 100: tol = 0.0005801768656714112\n",
      "iters 150: tol = 0.00020449812755515828\n",
      "iters 50: tol = 0.0011368981995533156\n",
      "iters 100: tol = 0.00035349105111026624\n",
      "iters 150: tol = 0.00013281159986133773\n",
      "2017-12-01 00:00:00 is done and took 165 iterations and 143.34 seconds\n",
      "iters 50: tol = 0.0031273868728260226\n",
      "iters 100: tol = 0.0009212059334419986\n",
      "iters 150: tol = 0.00027288000011051194\n",
      "iters 50: tol = 0.004487713796094028\n",
      "iters 100: tol = 0.0017478101253030898\n",
      "iters 150: tol = 0.0007081621626992551\n",
      "iters 200: tol = 0.00029625179855871653\n",
      "iters 250: tol = 0.00013085601338724828\n",
      "iters 50: tol = 0.0012533294755860958\n",
      "iters 100: tol = 0.00029767381733236675\n",
      "iters 50: tol = 0.0017808025446500175\n",
      "iters 100: tol = 0.0005588376829142039\n",
      "iters 150: tol = 0.0002310378778112021\n",
      "iters 200: tol = 0.00010550570662393177\n",
      "iters 50: tol = 0.0023361701560761228\n",
      "iters 100: tol = 0.0012405732965595773\n",
      "iters 150: tol = 0.0008425710219855942\n",
      "iters 200: tol = 0.0006179199560543136\n",
      "iters 250: tol = 0.0004755213823660065\n",
      "iters 300: tol = 0.0003811064176843515\n",
      "iters 350: tol = 0.00031199067606968445\n",
      "iters 400: tol = 0.00025949093800703427\n",
      "iters 450: tol = 0.00021845810799470122\n",
      "iters 500: tol = 0.00018565847086210452\n",
      "iters 550: tol = 0.0001589656840000897\n",
      "iters 600: tol = 0.00013692673575504966\n",
      "iters 650: tol = 0.00011851557143960356\n",
      "iters 700: tol = 0.00010298675705555649\n",
      "iters 50: tol = 0.006545684180799205\n",
      "iters 100: tol = 0.003587182968343777\n",
      "iters 150: tol = 0.001206098093634922\n",
      "iters 200: tol = 0.00034578231220194766\n",
      "iters 50: tol = 0.0020490129108252098\n",
      "iters 100: tol = 0.0004169811254471911\n",
      "iters 50: tol = 0.0007692509666488156\n",
      "iters 100: tol = 0.00011965560437499079\n",
      "iters 50: tol = 0.0017944654613086808\n",
      "iters 100: tol = 0.0008266265923955185\n",
      "iters 150: tol = 0.0003840050120513805\n",
      "iters 200: tol = 0.0001820995266608172\n",
      "iters 50: tol = 0.0030565144074017336\n",
      "iters 100: tol = 0.0020831822708877734\n",
      "iters 150: tol = 0.001406089411833955\n",
      "iters 200: tol = 0.0009113987529948409\n",
      "iters 250: tol = 0.0005741688661000666\n",
      "iters 300: tol = 0.0003542701265775161\n",
      "iters 350: tol = 0.00021548870756915584\n",
      "iters 400: tol = 0.0001298635226361089\n",
      "iters 50: tol = 0.0019786593092242233\n",
      "iters 100: tol = 0.0005357867671740291\n",
      "iters 150: tol = 0.00014200258204022376\n",
      "iters 50: tol = 0.0002595445872354274\n",
      "2018-12-01 00:00:00 is done and took 77 iterations and 69.54 seconds\n",
      "iters 50: tol = 0.0014074070825212948\n",
      "iters 100: tol = 0.0006315743419743614\n",
      "iters 150: tol = 0.00030634239920990236\n",
      "iters 200: tol = 0.00014862814492466736\n",
      "iters 50: tol = 0.0030323383072245746\n",
      "iters 100: tol = 0.0010575696332999485\n",
      "iters 150: tol = 0.00035398735672903525\n",
      "iters 200: tol = 0.00011511848383355394\n",
      "iters 50: tol = 0.0007707068569242082\n",
      "iters 100: tol = 0.00028522117378093625\n",
      "iters 150: tol = 0.00010254078572624614\n",
      "iters 50: tol = 0.00245911694457307\n",
      "iters 100: tol = 0.0006511165200833238\n",
      "iters 150: tol = 0.00020805970621510378\n",
      "iters 50: tol = 0.0014002732822381292\n",
      "iters 100: tol = 0.000565233509173868\n",
      "iters 150: tol = 0.0002288115077898334\n",
      "iters 50: tol = 0.0012415999100144903\n",
      "iters 100: tol = 0.00062722676744012\n",
      "iters 150: tol = 0.00028302717395956023\n",
      "iters 200: tol = 0.00012678602361532176\n",
      "iters 50: tol = 0.0005471760673958492\n",
      "iters 100: tol = 0.00023783096220025834\n",
      "iters 150: tol = 0.00010048752699143293\n",
      "iters 50: tol = 0.0003439777118227072\n",
      "iters 50: tol = 0.00037960165431893955\n",
      "iters 100: tol = 0.0001093192209213889\n",
      "iters 50: tol = 0.0017718092739404945\n",
      "iters 100: tol = 0.0008627458309654404\n",
      "iters 150: tol = 0.000473698959042727\n",
      "iters 200: tol = 0.00026839417845795494\n",
      "iters 250: tol = 0.00015387657635540508\n",
      "iters 50: tol = 0.0010264979008598601\n",
      "iters 100: tol = 0.0002533132047067843\n",
      "iters 150: tol = 0.00019928260794325765\n",
      "iters 200: tol = 0.00017976589331685378\n",
      "iters 250: tol = 0.0001461388122725709\n",
      "iters 300: tol = 0.00011506670536992267\n",
      "iters 50: tol = 0.0015801216254102551\n",
      "iters 100: tol = 0.0011262224053573446\n",
      "iters 150: tol = 0.0008115379043529131\n",
      "iters 200: tol = 0.0005937842701072282\n",
      "iters 250: tol = 0.0004394430762700019\n",
      "iters 300: tol = 0.00032866961890992696\n",
      "iters 350: tol = 0.0002482020409962371\n",
      "iters 400: tol = 0.00018901085825873132\n",
      "iters 450: tol = 0.0001449443256150429\n",
      "iters 500: tol = 0.00011178511451434558\n",
      "2019-12-01 00:00:00 is done and took 522 iterations and 478.12 seconds\n",
      "iters 50: tol = 0.0022737626705129355\n",
      "iters 100: tol = 0.0013303960092401201\n",
      "iters 150: tol = 0.0009638783194904355\n",
      "iters 200: tol = 0.0007963925725593263\n",
      "iters 250: tol = 0.0007099457808763185\n",
      "iters 300: tol = 0.0006620076343860151\n",
      "iters 350: tol = 0.0006412115731461304\n",
      "iters 400: tol = 0.0006398093220189305\n",
      "iters 450: tol = 0.0006491528035672267\n",
      "iters 500: tol = 0.0006802800275146903\n",
      "iters 550: tol = 0.0007232558170267083\n",
      "iters 600: tol = 0.0007738481481268239\n",
      "iters 650: tol = 0.0008293780888896651\n",
      "iters 700: tol = 0.0008853741958342598\n",
      "iters 750: tol = 0.0009409769643918542\n",
      "iters 800: tol = 0.0009920137389741457\n",
      "iters 850: tol = 0.0010179237055270851\n",
      "iters 900: tol = 0.0010087464138447966\n",
      "iters 950: tol = 0.000959699636047584\n",
      "iters 1000: tol = 0.0008739184600731753\n",
      "iters 1050: tol = 0.0007619948922046316\n",
      "iters 1100: tol = 0.0006383121790799473\n",
      "iters 1150: tol = 0.0005164364904542418\n",
      "iters 1200: tol = 0.00040601941460827184\n",
      "iters 1250: tol = 0.0003120328544835016\n",
      "iters 1300: tol = 0.00023563989036362587\n",
      "iters 1350: tol = 0.00017561605993426932\n",
      "iters 1400: tol = 0.00012960496367603325\n",
      "iters 50: tol = 0.010147643469394008\n",
      "iters 100: tol = 0.003933833081928753\n",
      "iters 150: tol = 0.00130448504736332\n",
      "iters 200: tol = 0.0005011813531183434\n",
      "iters 250: tol = 0.0002256299668189632\n",
      "iters 300: tol = 0.00012305710099047573\n",
      "iters 50: tol = 0.006925899273331426\n",
      "iters 100: tol = 0.0037313271047099833\n",
      "iters 150: tol = 0.0016057749718332293\n",
      "iters 200: tol = 0.0006099622722766929\n",
      "iters 250: tol = 0.00022511167173205893\n",
      "iters 50: tol = 0.001617037966119561\n",
      "iters 100: tol = 0.000339603807169242\n",
      "iters 150: tol = 0.0001279996359558888\n",
      "iters 50: tol = 0.003998772732001665\n",
      "iters 100: tol = 0.001394514709452288\n",
      "iters 150: tol = 0.0007733433943510892\n",
      "iters 200: tol = 0.0004182435348596414\n",
      "iters 250: tol = 0.0002134836081777447\n",
      "iters 300: tol = 0.00010568199842508896\n",
      "iters 50: tol = 0.002261931494849545\n",
      "iters 100: tol = 0.0009637483884985809\n",
      "iters 150: tol = 0.00040752408826082165\n",
      "iters 200: tol = 0.00018761695207687723\n",
      "iters 50: tol = 0.002455858865535898\n",
      "iters 100: tol = 0.0009008308477989058\n",
      "iters 150: tol = 0.0004804649591915733\n",
      "iters 200: tol = 0.00027454691455147673\n",
      "iters 250: tol = 0.00015556789483639477\n",
      "iters 50: tol = 0.0012059432090860689\n",
      "iters 100: tol = 0.00036998045064629004\n",
      "iters 150: tol = 0.00015902380077159606\n",
      "iters 50: tol = 0.001140401061786811\n",
      "iters 100: tol = 0.00041084762507992423\n",
      "iters 150: tol = 0.00016291873956664205\n",
      "iters 50: tol = 0.0010248504162064243\n",
      "iters 100: tol = 0.0002870989621437947\n",
      "iters 50: tol = 0.0011243006572714265\n",
      "iters 100: tol = 0.00027836347108811665\n",
      "iters 50: tol = 0.0007638261655565359\n",
      "iters 100: tol = 0.00023099119753213371\n",
      "2020-12-01 00:00:00 is done and took 137 iterations and 128.09 seconds\n",
      "iters 50: tol = 0.0013389411983387722\n",
      "iters 100: tol = 0.00047729431358023433\n",
      "iters 150: tol = 0.00014870124148536057\n",
      "iters 50: tol = 0.0004978362543457404\n",
      "iters 100: tol = 0.0001861006677326138\n",
      "iters 50: tol = 0.0007041324059750276\n",
      "iters 50: tol = 0.0023386544557391087\n",
      "iters 100: tol = 0.0009722992483989223\n",
      "iters 150: tol = 0.0003993189672883979\n",
      "iters 200: tol = 0.0001630093273539135\n",
      "iters 50: tol = 0.0005735262985167022\n",
      "iters 100: tol = 0.0001938925725479823\n",
      "iters 50: tol = 0.00013540887516755307\n",
      "iters 50: tol = 0.00045803182216253013\n",
      "iters 100: tol = 0.00013325637653482936\n",
      "iters 50: tol = 0.012764112290726659\n",
      "iters 100: tol = 0.002911979961158162\n",
      "iters 150: tol = 0.0005643315065773646\n",
      "iters 200: tol = 0.00012486829662572418\n",
      "iters 50: tol = 0.0030954543936417\n",
      "iters 100: tol = 0.000741106452175358\n",
      "iters 150: tol = 0.00017435375458750568\n",
      "iters 50: tol = 0.016237643172082872\n",
      "iters 100: tol = 0.004257110877238984\n",
      "iters 150: tol = 0.001072175846017008\n",
      "iters 200: tol = 0.00031967093425195464\n",
      "iters 250: tol = 0.00010104669335118076\n",
      "iters 50: tol = 0.005126933896540398\n",
      "iters 100: tol = 0.001308907235160639\n",
      "iters 150: tol = 0.00033346386864924193\n",
      "iters 50: tol = 0.00035429677296677786\n",
      "2021-12-01 00:00:00 is done and took 96 iterations and 152.96 seconds\n",
      "iters 50: tol = 0.0016854290894368074\n",
      "iters 100: tol = 0.0008406150962223524\n",
      "iters 150: tol = 0.00042518327738960693\n",
      "iters 200: tol = 0.00021347522479642222\n",
      "iters 250: tol = 0.00010664938828797155\n",
      "iters 50: tol = 0.0007839216955551342\n",
      "iters 100: tol = 0.0005406888485888217\n",
      "iters 150: tol = 0.00038273829512580626\n",
      "iters 200: tol = 0.00026577743512645746\n",
      "iters 250: tol = 0.0001837951273752081\n",
      "iters 300: tol = 0.0001272584723190917\n",
      "iters 50: tol = 0.025014948708546758\n",
      "iters 100: tol = 0.011499896427077605\n",
      "iters 150: tol = 0.0025671023530025594\n",
      "iters 200: tol = 0.0010256975257880718\n",
      "iters 250: tol = 0.0007338471423570248\n",
      "iters 300: tol = 0.0007866437357979539\n",
      "iters 350: tol = 0.0010142062916276284\n",
      "iters 400: tol = 0.0013356036647191871\n",
      "iters 450: tol = 0.0018623221764229037\n",
      "iters 500: tol = 0.0028127182833952435\n",
      "iters 550: tol = 0.004660042856767133\n",
      "iters 600: tol = 0.010107235172833295\n",
      "iters 650: tol = 0.02133063284206166\n",
      "iters 700: tol = 0.027587061455178974\n",
      "iters 750: tol = 0.00992556151304691\n",
      "iters 800: tol = 0.004059197192671338\n",
      "iters 850: tol = 0.0012969737497772194\n",
      "iters 900: tol = 0.00041323481310673316\n",
      "iters 950: tol = 0.00014369899272381748\n",
      "iters 50: tol = 0.001648546911940052\n",
      "iters 100: tol = 0.0009402706647851922\n",
      "iters 150: tol = 0.0006422835727365045\n",
      "iters 200: tol = 0.00042314951594023265\n",
      "iters 250: tol = 0.0002805838683388323\n",
      "iters 300: tol = 0.00018840368080957903\n",
      "iters 350: tol = 0.0001277819689398385\n",
      "iters 50: tol = 0.0012003322187969512\n",
      "iters 100: tol = 0.00011705934211425628\n",
      "iters 50: tol = 0.0011077969975534785\n",
      "iters 100: tol = 0.0010546027150410975\n",
      "iters 150: tol = 0.0009678307072986669\n",
      "iters 200: tol = 0.0008436182262437919\n",
      "iters 250: tol = 0.0007100557507370153\n",
      "iters 300: tol = 0.0005833918514294156\n",
      "iters 350: tol = 0.000471089982049977\n",
      "iters 400: tol = 0.0003757269921959294\n",
      "iters 450: tol = 0.00029704370531077884\n",
      "iters 500: tol = 0.00023337425689609614\n",
      "iters 550: tol = 0.00018253574773263725\n",
      "iters 600: tol = 0.000142315108725255\n",
      "iters 650: tol = 0.00011069956357673272\n",
      "iters 50: tol = 0.004532614492227416\n",
      "iters 100: tol = 0.0052986895190133\n",
      "iters 150: tol = 0.004026492424667016\n",
      "iters 200: tol = 0.002899029427156141\n",
      "iters 250: tol = 0.0020836857092384475\n",
      "iters 300: tol = 0.0014517519460529593\n",
      "iters 350: tol = 0.0009929604393147384\n",
      "iters 400: tol = 0.000673748170473637\n",
      "iters 450: tol = 0.00045563896692454864\n",
      "iters 500: tol = 0.0003077409409245613\n",
      "iters 550: tol = 0.0002077599755552495\n",
      "iters 600: tol = 0.00014024843465462733\n",
      "iters 50: tol = 0.004596403565226503\n",
      "iters 100: tol = 0.002738858424595092\n",
      "iters 150: tol = 0.0014386466850327961\n",
      "iters 200: tol = 0.0006515213393518682\n",
      "iters 250: tol = 0.00027704636615866196\n",
      "iters 300: tol = 0.00011486757360101851\n",
      "iters 50: tol = 0.0032346455284154585\n",
      "iters 100: tol = 0.0011538365893806746\n",
      "iters 150: tol = 0.00038397314414001515\n",
      "iters 200: tol = 0.00012167260413181724\n",
      "iters 50: tol = 0.004348575341053484\n",
      "iters 100: tol = 0.000678850244871132\n",
      "iters 150: tol = 0.00014175467039878598\n",
      "iters 50: tol = 0.0020490654047482515\n",
      "iters 100: tol = 0.00045404293937939544\n",
      "iters 150: tol = 0.0001684511773119013\n",
      "iters 50: tol = 0.0010295609870650813\n",
      "iters 100: tol = 0.00026214107332361847\n",
      "2022-12-01 00:00:00 is done and took 135 iterations and 133.12 seconds\n",
      "iters 50: tol = 0.0008923686662739017\n",
      "iters 100: tol = 0.00016800973002439878\n",
      "iters 50: tol = 0.0009625340143055161\n",
      "iters 100: tol = 0.00013454409291899228\n",
      "iters 50: tol = 0.001457661379783426\n",
      "iters 100: tol = 0.00014272490581102026\n",
      "iters 50: tol = 0.0013317685176140737\n",
      "iters 100: tol = 0.00021644112962881934\n",
      "iters 50: tol = 0.008776730874115755\n",
      "iters 100: tol = 0.0027140145684114558\n",
      "iters 150: tol = 0.0011789566092793718\n",
      "iters 200: tol = 0.000519586622555579\n",
      "iters 250: tol = 0.00024085153657371627\n",
      "iters 300: tol = 0.0001121873480163238\n",
      "iters 50: tol = 0.0022784129747372983\n",
      "iters 100: tol = 0.001114327637506407\n",
      "iters 150: tol = 0.0007431512754474956\n",
      "iters 200: tol = 0.0005753326514064128\n",
      "iters 250: tol = 0.00047254918384781464\n",
      "iters 300: tol = 0.0004100696428556705\n",
      "iters 350: tol = 0.0003740677400324022\n",
      "iters 400: tol = 0.00035737941249477934\n",
      "iters 450: tol = 0.000356884234221283\n",
      "iters 500: tol = 0.0003723753756637582\n",
      "iters 550: tol = 0.00040841790249473986\n",
      "iters 600: tol = 0.000476675776301394\n",
      "iters 650: tol = 0.0005884801683224972\n",
      "iters 700: tol = 0.000776253643739816\n",
      "iters 750: tol = 0.0011109572231094988\n",
      "iters 800: tol = 0.001765516556543667\n",
      "iters 850: tol = 0.003223859123724415\n",
      "iters 900: tol = 0.007150268463884543\n",
      "iters 950: tol = 0.03626737285557685\n",
      "iters 1000: tol = 0.005235121382821228\n",
      "iters 1050: tol = 0.005643464473513715\n",
      "iters 1100: tol = 0.0031077176279209473\n",
      "iters 1150: tol = 0.0014312624420993458\n",
      "iters 1200: tol = 0.0006679764864423454\n",
      "iters 1250: tol = 0.0003225831109989752\n",
      "iters 1300: tol = 0.00016023788725577637\n",
      "iters 50: tol = 0.00300375185953472\n",
      "iters 100: tol = 0.0011246984768439328\n",
      "iters 150: tol = 0.00044304681927342937\n",
      "iters 200: tol = 0.00018274014081753887\n",
      "iters 50: tol = 0.0023751042542300427\n",
      "iters 100: tol = 0.000463470633691343\n",
      "iters 50: tol = 0.001341592011578907\n",
      "iters 100: tol = 0.00045557763262149553\n",
      "iters 150: tol = 0.00014677669622031875\n",
      "iters 50: tol = 0.0017852688244330839\n",
      "iters 100: tol = 0.0005595541290243089\n",
      "iters 150: tol = 0.00018383079922801304\n",
      "iters 50: tol = 0.0018532952180413398\n",
      "iters 100: tol = 0.0004698181496081699\n",
      "iters 150: tol = 0.00015452750417610517\n",
      "iters 50: tol = 0.0019279103189401752\n",
      "iters 100: tol = 0.00044518187448928936\n",
      "iters 150: tol = 0.00012556012805067795\n",
      "2023-12-01 00:00:00 is done and took 160 iterations and 183.53 seconds\n",
      "iters 50: tol = 0.0017256528753282555\n",
      "iters 100: tol = 0.000636691727140315\n",
      "iters 150: tol = 0.0002553103452029859\n",
      "iters 200: tol = 0.00010904322546079204\n",
      "iters 50: tol = 0.0020940892600460614\n",
      "iters 100: tol = 0.0002778469776729686\n",
      "iters 50: tol = 0.0016273641576503017\n",
      "iters 100: tol = 0.0017363799420081483\n",
      "iters 150: tol = 0.002013402359125449\n",
      "iters 200: tol = 0.0028481643272205703\n",
      "iters 250: tol = 0.004031416299784318\n",
      "iters 300: tol = 0.0069177183273746445\n",
      "iters 350: tol = 0.012420372348139294\n",
      "iters 400: tol = 0.015941696420054385\n",
      "iters 450: tol = 0.009133293714503748\n",
      "iters 500: tol = 0.0029387870811419248\n",
      "iters 550: tol = 0.001895823854918044\n",
      "iters 600: tol = 0.0008808520075562765\n",
      "iters 650: tol = 0.0003701990463713667\n",
      "iters 700: tol = 0.00015253355907085542\n",
      "iters 50: tol = 0.0003705934107262887\n",
      "iters 50: tol = 0.0017053173040619818\n",
      "iters 100: tol = 0.0005067050616569535\n",
      "iters 150: tol = 0.00018004164561060895\n",
      "iters 50: tol = 0.007810794243863661\n",
      "iters 100: tol = 0.002826532778881452\n",
      "iters 150: tol = 0.0010918178313155114\n",
      "iters 200: tol = 0.00044737846595377384\n",
      "iters 250: tol = 0.0001911272305885614\n",
      "iters 50: tol = 0.007969657105043326\n",
      "iters 100: tol = 0.0019069659162163077\n",
      "iters 150: tol = 0.0011567630043436417\n",
      "iters 200: tol = 0.000757882107196628\n",
      "iters 250: tol = 0.0004850855884462879\n",
      "iters 300: tol = 0.0003065068156097306\n",
      "iters 350: tol = 0.00019313467596265843\n",
      "iters 400: tol = 0.00012159042972598177\n",
      "\n",
      "==================================================\n",
      "EVALUATING IPCA FACTORS FOR EARNINGS PREDICTION\n",
      "==================================================\n",
      "Factor loadings shape: (149, 8)\n",
      "Top characteristics for each factor:\n",
      "\n",
      "Factor 0:\n",
      "  Constant: 0.384\n",
      "  op_at: 0.380\n",
      "  op_atl1: 0.338\n",
      "  gp_atl1: 0.329\n",
      "  gp_at: 0.266\n",
      "\n",
      "Factor 1:\n",
      "  dolvol_126d: 0.371\n",
      "  at_turnover: 0.319\n",
      "  ami_126d: 0.257\n",
      "  sale_me: 0.230\n",
      "  eqnpo_12m: 0.219\n",
      "\n",
      "Factor 2:\n",
      "  cop_at: 0.303\n",
      "  qmj_safety: 0.293\n",
      "  qmj_prof: 0.270\n",
      "  cop_atl1: 0.265\n",
      "  gp_atl1: 0.240\n",
      "\n",
      "Factor 3:\n",
      "  turnover_126d: 0.499\n",
      "  zero_trades_126d: 0.312\n",
      "  Constant: 0.282\n",
      "  market_equity: 0.258\n",
      "  gp_at: 0.180\n",
      "\n",
      "Factor 4:\n",
      "  ami_126d: 0.353\n",
      "  turnover_126d: 0.242\n",
      "  gp_atl1: 0.234\n",
      "  at_gr1: 0.207\n",
      "  zero_trades_126d: 0.203\n",
      "\n",
      "Factor 5:\n",
      "  mispricing_mgmt: 0.266\n",
      "  qmj: 0.244\n",
      "  eqnpo_me: 0.217\n",
      "  noa_at: 0.216\n",
      "  debt_me: 0.213\n",
      "\n",
      "Factor 6:\n",
      "  Constant: 0.355\n",
      "  qmj: 0.255\n",
      "  op_atl1: 0.224\n",
      "  o_score: 0.221\n",
      "  turnover_126d: 0.200\n",
      "\n",
      "Factor 7:\n",
      "  mispricing_perf: 0.234\n",
      "  Constant: 0.221\n",
      "  inv_gr1: 0.208\n",
      "  nncoa_gr1a: 0.206\n",
      "  cop_at: 0.201\n",
      "\n",
      "Factor scores dataset: 127199 observations\n",
      "Train set: 79917 observations\n",
      "Test set: 47282 observations\n",
      "\n",
      "PREDICTION RESULTS:\n",
      "Accuracy: 0.576\n",
      "Precision: 0.653\n",
      "Recall: 0.591\n",
      "F1-Score: 0.620\n",
      "AUC-ROC: 0.600\n",
      "\n",
      "FACTOR IMPORTANCE (XGBoost):\n",
      "0: 0.286\n",
      "1: 0.102\n",
      "2: 0.112\n",
      "3: 0.106\n",
      "4: 0.099\n",
      "5: 0.096\n",
      "6: 0.099\n",
      "7: 0.100\n",
      "\n",
      "==================================================\n",
      "BACKTESTING IPCA EARNINGS SURPRISE STRATEGY\n",
      "==================================================\n",
      "\n",
      "PERFORMANCE METRICS:\n",
      "                       Annual Return (%) Annualized Volatility (%)  \\\n",
      "Benchmark                           1.32                      0.45   \n",
      "IPCA_Earnings_Strategy             15.54                     28.98   \n",
      "\n",
      "                       Annualized Sharpe Ratio  \\\n",
      "Benchmark                                2.912   \n",
      "IPCA_Earnings_Strategy                   0.536   \n",
      "\n",
      "                       Annualized Alpha vs Benchmark (%) Beta vs Benchmark  \\\n",
      "Benchmark                                           0.00               nan   \n",
      "IPCA_Earnings_Strategy                             17.04               nan   \n",
      "\n",
      "                       Max Drawdown (%) Max Monthly Loss (%)  \\\n",
      "Benchmark                          0.00                 0.00   \n",
      "IPCA_Earnings_Strategy           -33.50               -21.35   \n",
      "\n",
      "                       Annualized Information Ratio vs Benchmark  \\\n",
      "Benchmark                                                    nan   \n",
      "IPCA_Earnings_Strategy                                     0.587   \n",
      "\n",
      "                       Annualized Tracking Error vs Benchmark (%)  \n",
      "Benchmark                                                    0.00  \n",
      "IPCA_Earnings_Strategy                                      29.02  \n"
     ]
    }
   ],
   "source": [
    "# 1. Load and prepare data\n",
    "filepath = 'goup_project_sample_v3.csv'  # Replace with your actual file path\n",
    "data, char_vars = prepare_earnings_surprise_data(filepath)\n",
    "\n",
    "# 2. Run IPCA with fixed K=8 (skip optimization for now)\n",
    "results, ipca, ipca_input = run_ipca_earnings_prediction(\n",
    "    data, char_vars, K=8, min_obs_per_date=100,\n",
    "    oos_start_year=2012, oos_window=60\n",
    ")\n",
    "\n",
    "# 3. Get predictions  \n",
    "prediction_results = evaluate_ipca_factors_for_prediction(\n",
    "    results, ipca_input, ipca, K=8, verbose=True\n",
    ")\n",
    "\n",
    "# 4. Run backtest\n",
    "if prediction_results:\n",
    "    backtest_results = backtest_ipca_strategy(\n",
    "        data, prediction_results, start_year=2015, end_year=2023, \n",
    "        top_n_stocks=50, hold_period=1\n",
    "    )\n",
    "    \n",
    "    # 5. Show performance\n",
    "    performance_summary = create_performance_summary(\n",
    "        backtest_results, strategy_name=\"IPCA_Earnings_Strategy\"\n",
    "    )\n",
    "    print(\"\\nPERFORMANCE METRICS:\")\n",
    "    print(performance_summary.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
